{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate aggregated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC\n",
    "\n",
    "FIXME\n",
    "\n",
    "FIXME\n",
    "\n",
    "FIXME\n",
    "\n",
    "FIXME\n",
    "\n",
    "FIXME\n",
    "\n",
    "FIXME\n",
    "\n",
    "* [1-A b](#1-A-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('.').absolute().joinpath('data')\n",
    "\n",
    "sales_train = pd.read_csv(data_dir.joinpath('sales_train.csv.gz'))\n",
    "sales_test = pd.read_csv(data_dir.joinpath('test.csv.gz'))\n",
    "items = pd.read_csv(data_dir.joinpath('items.csv'))\n",
    "item_categories = pd.read_csv(data_dir.joinpath('item_categories.csv'))\n",
    "shops = pd.read_csv(data_dir.joinpath('shops.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples = sales_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast the dates to actual dates for easier manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train.loc[:, 'date'] = pd.to_datetime(sales_train.loc[:, 'date'], format='%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After investigating the item count per day outliners we saw that these may actual be correct (and not arising from typos etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we saw that the outliner in price could be fixed, by converting `'Radmin 3  - 522 лиц.'` to `'Radmin 3  - 1 лиц.'`.\n",
    "\n",
    "We do the conversion in the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values obtained from EDA\n",
    "item_count_522 = 522\n",
    "item_id_1 = 6065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_price = sales_train.loc[:, 'item_price'].max()\n",
    "high_price = sales_train.loc[sales_train.loc[:, 'item_price'] == max_price]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = high_price.index[0]\n",
    "\n",
    "sales_train.loc[index, 'item_id'] = item_id_1\n",
    "sales_train.loc[index, 'item_cnt_day'] = item_count_522\n",
    "sales_train.loc[index, 'item_price'] = max_price/item_count_522"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we saw that the datapoint for plastic bags had a high item count. As this is used to calculate the renevue below, we will not alter this item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen that that `item_cnt_day` are of floats, to speed up calculations, we transform them to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train.loc[:, 'item_cnt_day'] = sales_train.loc[:, 'item_cnt_day'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the base set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here use the trick from the ensembling exercise where we create an outer product of the `item_id` and `shop_id` present in each block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we are predicting for the `34`th month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test.loc[:, 'date_block_num'] = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "cols = ['date_block_num','shop_id','item_id']\n",
    "\n",
    "# The training part\n",
    "for block in sales_train.loc[:,'date_block_num'].unique():\n",
    "    tmp = sales_train.loc[sales_train.loc[:, 'date_block_num'] == block, cols]\n",
    "    # NOTE: Here we make an outer product of 'date_block_num','shop_id' and 'item_id'\n",
    "    data.append(np.array(list(product([block], tmp.loc[:,'shop_id'].unique(), tmp.loc[:,'item_id'].unique()))))\n",
    "    \n",
    "# Make a sorted dataset of the list\n",
    "data = pd.DataFrame(np.vstack(data), columns=cols).sort_values(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to expand the test set, as we will use this for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, sales_test.loc[:, ['date_block_num','shop_id','item_id']]], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating aggregated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding shop features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_drop_cols = ['item_price', 'item_cnt_day', 'item_category_id', 'revenue']\n",
    "shop = pd.merge(sales_train, items.loc[:, ['item_id', 'item_category_id']], how='left', on=['item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop.loc[:, 'revenue'] = shop.loc[:, 'item_price'] * shop.loc[:, 'item_cnt_day']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_sum = shop.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_sum.loc[:, 'month_shop_revenue_sum'] = \\\n",
    "    shop_sum.loc[:, ['date_block_num', 'shop_id', 'revenue']].\\\n",
    "    groupby(['date_block_num', 'shop_id'])['revenue'].transform(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_sum.loc[:, 'month_shop_item_id_item_cnt_sum'] = \\\n",
    "    shop_sum.loc[:, ['date_block_num', 'shop_id', 'item_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'shop_id', 'item_id'])['item_cnt_day'].transform(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_sum.loc[:, 'month_shop_item_cnt_sum'] = \\\n",
    "    shop_sum.loc[:, ['date_block_num', 'shop_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'shop_id'])['item_cnt_day'].transform(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_sum.drop(shop_drop_cols, axis=1, inplace=True)\n",
    "shop_sum.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_mean = shop.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_mean.loc[:, 'shop_revenue_avg'] = \\\n",
    "    shop_mean.loc[:, ['shop_id', 'revenue']].\\\n",
    "    groupby(['shop_id'])['revenue'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_mean.loc[:, 'month_shop_item_cnt_avg'] = \\\n",
    "    shop_mean.loc[:, ['date_block_num', 'shop_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'shop_id'])['item_cnt_day'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_mean.loc[:, 'month_shop_item_id_item_cnt_avg'] = \\\n",
    "    shop_mean.loc[:, ['date_block_num', 'shop_id', 'item_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'shop_id', 'item_id'])['item_cnt_day'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_mean.loc[:, 'month_shop_item_cat_item_cnt_avg'] = \\\n",
    "    shop_mean.loc[:, ['date_block_num', 'shop_id', 'item_category_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'shop_id', 'item_category_id'])['item_cnt_day'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_mean.drop(shop_drop_cols, axis=1, inplace=True)\n",
    "shop_mean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_other = shop.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We use a slightly different notation on this feature as it can be merged on shop instead on shop-item\n",
    "shop_other.loc[:, 'shop_unique_items_len'] = \\\n",
    "    shop_other.loc[:, ['shop_id', 'item_id']].\\\n",
    "    groupby('shop_id')['item_id'].transform(lambda item_id_group: len(np.unique(item_id_group)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_other.loc[:, 'month_shop_item_id_item_cnt_max'] = \\\n",
    "    shop_other.loc[:, ['date_block_num', 'shop_id', 'item_id', 'item_cnt_day']].\\\n",
    "    groupby(['shop_id', 'item_id', 'date_block_num'])['item_cnt_day'].transform(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_other.loc[:, 'month_shop_item_id_item_cnt_min'] = \\\n",
    "    shop_other.loc[:, ['date_block_num', 'shop_id', 'item_id', 'item_cnt_day']].\\\n",
    "    groupby(['shop_id', 'item_id', 'date_block_num'])['item_cnt_day'].transform(min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_other.drop(shop_drop_cols, axis=1, inplace=True)\n",
    "shop_other.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding non-shop features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shop_drop_cols = ['item_price', 'item_category_id', 'item_cnt_day']\n",
    "non_shop = pd.merge(sales_train, items.loc[:, ['item_id', 'item_category_id']], how='left', on=['item_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregates by item and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shop.loc[:, 'month_item_id_item_cnt_sum'] = \\\n",
    "    non_shop.loc[:, ['date_block_num', 'item_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'item_id'])['item_cnt_day'].transform(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shop.loc[:, 'month_item_id_item_cnt_avg'] = \\\n",
    "    non_shop.loc[:, ['date_block_num', 'item_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'item_id'])['item_cnt_day'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shop.loc[:, 'month_item_cat_item_cnt_avg'] = \\\n",
    "    non_shop.loc[:, ['date_block_num', 'item_category_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'item_category_id'])['item_cnt_day'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shop.loc[:, 'month_item_cnt_avg'] = \\\n",
    "    non_shop.loc[:, ['date_block_num', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num'])['item_cnt_day'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shop.loc[:, 'month_item_id_item_cnt_max'] = \\\n",
    "    non_shop.loc[:, ['date_block_num', 'item_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'item_id'])['item_cnt_day'].transform(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shop.loc[:, 'month_item_id_item_cnt_min'] = \\\n",
    "    non_shop.loc[:, ['date_block_num', 'item_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'item_id'])['item_cnt_day'].transform(min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shop.drop(non_shop_drop_cols, axis=1, inplace=True)\n",
    "non_shop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge and clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_sum.drop_duplicates(inplace=True)\n",
    "shop_mean.drop_duplicates(inplace=True)\n",
    "shop_other.drop_duplicates(inplace=True)\n",
    "non_shop.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date', 'date_block_num', 'shop_id', 'item_id']\n",
    "\n",
    "aggregates = pd.merge(shop_sum, shop_mean, how='left', on=merge_on)\n",
    "aggregates = pd.merge(aggregates, shop_other, how='left', on=merge_on)\n",
    "aggregates = pd.merge(aggregates, non_shop, how='left', on=merge_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that we didn't introduce any `NaN`s or that we accidentally expanded the set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if aggregates.isnull().any().any():\n",
    "    raise AssertionError('NaNs were created')\n",
    "    \n",
    "n_aggregates = aggregates.shape[0]\n",
    "if n_aggregates > n_train_samples:\n",
    "    raise AssertionError(f'The training set was expanded: '\n",
    "                         f'n_aggregates={n_aggregates} and n_train_samples={n_train_samples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have no longer use of the day information of the aggregates, so we remove this and remove the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregates.drop('date', axis=1, inplace=True)\n",
    "aggregates.drop_duplicates(inplace=True)\n",
    "aggregates.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape = {aggregates.shape}')\n",
    "aggregates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rejoin `item_category_id` as we will use this as a categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregates = pd.merge(aggregates, items.loc[:, ['item_id', 'item_category_id']], how='left', on='item_id')\n",
    "print(f'Shape = {aggregates.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clipping the target value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this comptetition the range of the predicted item count sold should be in the range $[0, 20]$.\n",
    "This leaves us with two choices:\n",
    "\n",
    "1. Clipping before training\n",
    "2. Clipping after prediction\n",
    "\n",
    "The disatvantage of 1. is that this will give us inconsistencies with other features like the revenue.\n",
    "However, features like the aggregated revenue of a shop is expected to influence the sales of a shop, so it is not that critical that we reduce the correlation between these features.\n",
    "\n",
    "On the other hand, if we go for option 2., the range of values which the model is trying to learn from is broad. This can lead to low performance as the target space becomes broad and therefore sparse.\n",
    "\n",
    "Because of this we clip prior to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregates.loc[:, 'month_shop_item_id_item_cnt_sum'].clip(0, 20, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we would like to check how the target variable is distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "aggregates.loc[:, 'month_shop_item_id_item_cnt_sum'].hist(ax =ax, bins=21)\n",
    "ax.set_xlabel('item_cnt_month')\n",
    "ax.set_ylabel('counts')\n",
    "ax.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del shop\n",
    "del shop_sum\n",
    "del shop_mean\n",
    "del shop_other\n",
    "del non_shop\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now added several aggregated features.\n",
    "In this section, we are in particular interested in exploring the relation between the categorical features and the target variable.\n",
    "\n",
    "This exploration will be the fundation to create mean encodings, where the purpose is to code the categorical features (with a lot of features) in such a way that the relation with the target variable is taken into account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'month_shop_item_id_item_cnt_sum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_item_id_item_cnt_pivot = \\\n",
    "    aggregates.pivot_table(index='shop_id',\n",
    "                           columns='item_id',\n",
    "                           values=target,\n",
    "                           aggfunc='count',\n",
    "                           fill_value=0)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(shop_item_id_item_cnt_pivot,\n",
    "            ax=ax,\n",
    "            cbar=True,\n",
    "            cmap='viridis', \n",
    "            cbar_kws={'label': target})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that certain items (for example around id $3429$ and $4279$) are sold broadly across all shops, that some shops have quite broad selection (between shop id $24$ and $33$).\n",
    "\n",
    "We see that with the current encoding, the shops are not clustred around certain items, but are scattered around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_item_cat_item_cnt_pivot = \\\n",
    "    aggregates.pivot_table(index='shop_id',\n",
    "                           columns='item_category_id',\n",
    "                           values=target,\n",
    "                           aggfunc='count',\n",
    "                           fill_value=0)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(shop_item_cat_item_cnt_pivot,\n",
    "            ax=ax,\n",
    "            cbar=True,\n",
    "            cmap='viridis', \n",
    "            cbar_kws={'label': target})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that a few categories are dominating the sales (like category $40$ and $56$), and the others are contributing less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_item_cat_item_cnt_pivot = \\\n",
    "    aggregates.pivot_table(index='item_id',\n",
    "                           columns='item_category_id',\n",
    "                           values=target,\n",
    "                           aggfunc='count',\n",
    "                           fill_value=0)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(item_id_item_cat_item_cnt_pivot,\n",
    "            ax=ax,\n",
    "            cbar=True,\n",
    "            cmap='viridis', \n",
    "            cbar_kws={'label': target})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not appear that the `item_id` is sorted in terms of `item_category_id`, hence will both provide information to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_date_block_num_item_cnt_pivot = \\\n",
    "    aggregates.pivot_table(index='shop_id',\n",
    "                           columns='date_block_num',\n",
    "                           values=target,\n",
    "                           aggfunc='count',\n",
    "                           fill_value=0)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(shop_date_block_num_item_cnt_pivot,\n",
    "            ax=ax,\n",
    "            cbar=True,\n",
    "            cmap='viridis', \n",
    "            cbar_kws={'label': target})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the EDA of the raw features, we see that we have seasonal trends, and that a few shops are dominating in terms of quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the investigation above, we can argue that it would make sense to create mean encodings for all the categorical features, as they all appear to bring new information to the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding mean encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now mean encode the categorical features above based on how often (on average) the target variable appears in the categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['item_id', 'shop_id', 'item_category_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = model_selection.KFold(n_splits=5, shuffle=False)\n",
    "new_features = []\n",
    "\n",
    "# Shortly told we will here aggregate a mean on the training set, \n",
    "# store it in the validation set and fill the missing values with the global mean\n",
    "for train_indices, valid_indices in kf.split(aggregates):\n",
    "    \n",
    "    # Train/validation split\n",
    "    train = aggregates.iloc[train_indices]\n",
    "    valid = aggregates.iloc[valid_indices]\n",
    "    \n",
    "    # Mean encoding\n",
    "    for feature in cat_features:\n",
    "        # NOTE: The lines below are equivalent to\n",
    "        # agg_mean = train.loc[:, [feature, target]].groupby(feature)[target].mean()\n",
    "        # mean_merged_on_valid_feature = \n",
    "        #     pd.merge(valid.loc[:, [feature]], agg_mean.to_frame(), how='left', on=feature).loc[:, feature]\n",
    "        mean_merged_on_valid_feature = valid.loc[:, feature].map(train.groupby(feature)[target].mean())\n",
    "    \n",
    "        # Store the results in aggregates\n",
    "        aggregates.loc[valid_indices, feature + '_mean_enc'] = mean_merged_on_valid_feature\n",
    "        \n",
    "global_mean = aggregates.loc[:, target].mean()\n",
    "aggregates.fillna(global_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate how correlated the mean encodings are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_enc_cols = [col for col in aggregates.columns if 'mean_enc' in col]\n",
    "corr_coef = aggregates.loc[:,[target, *mean_enc_cols]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_coef.loc[mean_enc_cols, target].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(aggregates.loc[:, 'item_id_mean_enc'], aggregates.loc[:, target], label='item_id', alpha=0.2)\n",
    "ax.scatter(aggregates.loc[:, 'shop_id_mean_enc'], aggregates.loc[:, target], label='shop_id', alpha=0.2)\n",
    "ax.scatter(aggregates.loc[:, 'item_category_id_mean_enc'], aggregates.loc[:, target], label='item_category_id', alpha=0.2)\n",
    "ax.set_ylabel('Target')\n",
    "ax.set_xlabel('Encoding')\n",
    "ax.legend(loc='best', fancybox=True, framealpha=0.5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the mean encodings are relatively well correlated with the target, and it doesn't appear that there is too much leakage.\n",
    "\n",
    "Notice, however that the categorical features may be multivalued with respect to the mean encoding (due to the k-fold validation).\n",
    "This becomes a problem when we will merge the encoded values to the test set.\n",
    "To solve this, we will aggregate a mean of the mean encoding and merge this with the data data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregates.loc[:, 'item_id_mean_mean_enc'] = \\\n",
    "    aggregates.loc[:, ['item_id_mean_enc', 'item_id']].\\\n",
    "    groupby(['item_id'])['item_id_mean_enc'].transform(np.mean)\n",
    "\n",
    "aggregates.loc[:, 'shop_id_mean_mean_enc'] = \\\n",
    "    aggregates.loc[:, ['shop_id_mean_enc', 'shop_id']].\\\n",
    "    groupby(['shop_id'])['shop_id_mean_enc'].transform(np.mean)\n",
    "\n",
    "aggregates.loc[:, 'item_category_id_mean_mean_enc'] = \\\n",
    "    aggregates.loc[:, ['item_category_id_mean_enc', 'item_category_id']].\\\n",
    "    groupby(['item_category_id'])['item_category_id_mean_enc'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mean_enc_cols = [col for col in aggregates.columns if 'mean_mean_enc' in col]\n",
    "corr_coef = aggregates.loc[:,[target, *mean_mean_enc_cols]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_coef.loc[mean_mean_enc_cols, target].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(aggregates.loc[:, 'item_id_mean_mean_enc'], aggregates.loc[:, target], label='item_id', alpha=0.2)\n",
    "ax.scatter(aggregates.loc[:, 'shop_id_mean_mean_enc'], aggregates.loc[:, target], label='shop_id', alpha=0.2)\n",
    "ax.scatter(aggregates.loc[:, 'item_category_id_mean_mean_enc'], aggregates.loc[:, target], label='item_category_id', alpha=0.2)\n",
    "ax.set_ylabel('Target')\n",
    "ax.set_xlabel('Encoding')\n",
    "ax.legend(loc='best', fancybox=True, framealpha=0.5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the main features of the main encoding is preserved.\n",
    "We therefore remove the original mean encoded features from the aggregated data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregates.drop(['item_id_mean_enc', 'shop_id_mean_enc', 'item_category_id_mean_enc'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the aggregated features with the data dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we make the temporal features, we start by mergning the aggregated features with the data dataframe in order to easily add the lagged features to the test set (month $34$ in the data data frame).\n",
    "\n",
    "**NOTE**: This section is long, and could probably be improved quite a bit. Essentially what happens is that we merge the different part of the aggregated data frame on the correct columns of the data data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_samples = data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by merging the item category id with the data data frame as we will later use item category as our merge-on column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['item_id']\n",
    "data_aggregate = pd.merge(data,\n",
    "                          items.loc[:, merge_on + ['item_category_id']],\n",
    "                          how='left', \n",
    "                          on=merge_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the different columns have different feature dependencies. We must ensure that we are merging the columns on the correct features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_cols = set(aggregates.columns)\n",
    "\n",
    "mean_enc_cols = [col for col in current_cols if 'mean_mean_enc' in col]\n",
    "\n",
    "current_cols -= set(mean_enc_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_shop_item_id_cols = [col for col in current_cols if \n",
    "                           'month' in col and\n",
    "                           'shop' in col and\n",
    "                           'item_id' in col]\n",
    "\n",
    "current_cols -= set(month_shop_item_id_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_shop_item_cat_cols = [col for col in current_cols if \n",
    "                            'month' in col and\n",
    "                            'shop' in col and\n",
    "                            'item_cat' in col]\n",
    "\n",
    "current_cols -= set(month_shop_item_cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_shop_cols = [col for col in current_cols if \n",
    "                   'month' in col and\n",
    "                   'shop' in col]\n",
    "\n",
    "current_cols -= set(month_shop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_item_id_cols = [col for col in current_cols if \n",
    "                      'month' in col and\n",
    "                      'item_id' in col]\n",
    "\n",
    "current_cols -= set(month_item_id_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_item_cat_cols = [col for col in current_cols if \n",
    "                       'month' in col and\n",
    "                       'item_cat' in col]\n",
    "\n",
    "current_cols -= set(month_item_cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_cols = [col for col in current_cols if \n",
    "              'month' in col]\n",
    "\n",
    "current_cols -= set(month_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_cols = [col for col in current_cols if \n",
    "            'shop' in col and\n",
    "             col != 'shop_id']\n",
    "\n",
    "identifier_cols = current_cols - set(shop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we did the correct thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if set(identifier_cols) != {'date_block_num', 'item_category_id', 'item_id', 'shop_id'}:\n",
    "    raise AssertionError('identifier_cols is not correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date_block_num', 'shop_id', 'item_id']\n",
    "data_aggregate = pd.merge(data_aggregate,\n",
    "                          aggregates.loc[:, merge_on + month_shop_item_id_cols].drop_duplicates(),\n",
    "                          how='left', \n",
    "                          on=merge_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have expanded our dataset with shop id - item id combinations that have not been sold during that month, we can safely replace the `NaN`s prior to week $34$ to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The inplace operator seem not to work (maybe because we are selecting a slice)\n",
    "#       Hence we use the assign operator\n",
    "data_aggregate.loc[data_aggregate.loc[:, 'date_block_num']<34, month_shop_item_id_cols] =\\\n",
    "    data_aggregate.loc[data_aggregate.loc[:, 'date_block_num']<34, month_shop_item_id_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date_block_num', 'shop_id', 'item_category_id']\n",
    "data_aggregate = pd.merge(data_aggregate,\n",
    "                          aggregates.loc[:, merge_on + month_shop_item_cat_cols].drop_duplicates(),\n",
    "                          how='left', \n",
    "                          on=merge_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the shop id - item id expansion, new combinations of shop id - item category id may appear. \n",
    "We can alson safely replace the `NaN`s prior to week $34$ to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The inplace operator seem not to work (maybe because we are selecting a slice)\n",
    "#       Hence we use the assign operator\n",
    "data_aggregate.loc[data_aggregate.loc[:, 'date_block_num']<34, month_shop_item_cat_cols] =\\\n",
    "    data_aggregate.loc[data_aggregate.loc[:, 'date_block_num']<34, month_shop_item_cat_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date_block_num', 'shop_id']\n",
    "data_aggregate = pd.merge(data_aggregate, \n",
    "                          aggregates.loc[:, merge_on + month_shop_cols].drop_duplicates(), \n",
    "                          how='left',\n",
    "                          on=merge_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No new `NaN`s should appear after the above merge as the expansion of the training set does not introduce new shops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date_block_num', 'item_id']\n",
    "data_aggregate = pd.merge(data_aggregate,\n",
    "                          aggregates.loc[:, merge_on + month_item_id_cols].drop_duplicates(),\n",
    "                          how='left',\n",
    "                          on=merge_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No new `NaN`s should appear after the above merge as the expansion of the training set does not introduce new items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date_block_num', 'item_category_id']\n",
    "data_aggregate = pd.merge(data_aggregate,\n",
    "                          aggregates.loc[:, merge_on + month_item_cat_cols].drop_duplicates(),\n",
    "                          how='left',\n",
    "                          on=merge_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No new `NaN`s should appear after the above merge as the expansion of the training set does not introduce new item categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date_block_num']\n",
    "data_aggregate = pd.merge(data_aggregate,\n",
    "                          aggregates.loc[:, merge_on + month_cols].drop_duplicates(), \n",
    "                          how='left',\n",
    "                          on=merge_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No new `NaN`s should appear after the above merge as the expansion of the training set does not introduce new months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['shop_id']\n",
    "data_aggregate = pd.merge(data_aggregate,\n",
    "                          aggregates.loc[:, merge_on + shop_cols].drop_duplicates(),\n",
    "                          how='left', \n",
    "                          on=merge_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No new `NaN`s should appear after the above merge as the expansion of the training set does not introduce new shops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge the mean encoded features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['shop_id']\n",
    "data_aggregate = pd.merge(data_aggregate, \n",
    "                          aggregates.loc[:, merge_on + ['shop_id_mean_mean_enc']].drop_duplicates(),\n",
    "                          how='left',\n",
    "                          on=merge_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No new `NaN`s should appear after the above merge as the expansion of the training set does not introduce new shops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['item_id']\n",
    "data_aggregate = pd.merge(data_aggregate, \n",
    "                          aggregates.loc[:, merge_on + ['item_id_mean_mean_enc']].drop_duplicates(),\n",
    "                          how='left',\n",
    "                          on=merge_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No new `NaN`s should appear after the above merge as the expansion of the training set does not introduce new items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['item_category_id']\n",
    "data_aggregate = pd.merge(data_aggregate, \n",
    "                          aggregates.loc[:, merge_on + ['item_category_id_mean_mean_enc']].drop_duplicates(),\n",
    "                          how='left',\n",
    "                          on=merge_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No new `NaN`s should appear after the above merge as the expansion of the training set does not introduce new categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that we didn't introduce any `NaN`s in the training set or that we accidentally expanded the set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_aggregate.loc[data_aggregate.loc[:, 'date_block_num']<34].isnull().any().any():\n",
    "    raise AssertionError('NaNs were created')\n",
    "    \n",
    "n_data_aggregate = data_aggregate.shape[0]\n",
    "if n_aggregates > n_data_samples:\n",
    "    raise AssertionError(f'The set was expanded: '\n",
    "                         f'n_aggregates={n_aggregates} and n_data_aggregate={n_data_aggregate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only two type of features should now contain `NaN`s in the test set:\n",
    "\n",
    "* Monthly features - as these are based on item counts not present in the test set\n",
    "* Item id features - as the test set contains new items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nan_features = data_aggregate.loc[data_aggregate.loc[:, 'date_block_num']==34].isnull().any()\n",
    "print(test_nan_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that the monthly features contains only `NaN`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_w_month = [col for col in data_aggregate.columns if 'month' in col]\n",
    "if not data_aggregate.loc[data_aggregate.loc[:, 'date_block_num']==34, cols_w_month].isnull().all().all():\n",
    "    raise AssertionError('The monthly features contained something else than NaNs')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to fill the `NaN` values for the non-month features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_col = [col for col, is_nan in zip(test_nan_features.index, test_nan_features.values) if is_nan]\n",
    "non_month_nan_cols = [col for col in data_aggregate.columns if col not in cols_w_month and col in nan_col]\n",
    "print(non_month_nan_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several options to fill these values.\n",
    "As we saw from the EDA of the raw data, only $1.6 %$ of the item ids were present in the test set that was not present in the training set. \n",
    "Thus, it should not matter too much what we fill these values with. \n",
    "We choose to use the mean encoding trick and fill them with the global mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aggregate.loc[:, 'item_id_mean_mean_enc'].fillna(global_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding temporal history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a sequential problem, we would like to incorperate some time information into the training set.\n",
    "\n",
    "Note that all the montly aggregated features we generated above will only be present as lagged features in the test set when we create the lagged features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lagged(df, col, lags, merge_on, fillna=0):\n",
    "    \"\"\"\n",
    "    Makes lagged features\n",
    "    \n",
    "    We make the lag this by adding the lag number to date_block_num\n",
    "    and merge the result on date_block_num of the corresponding month of the input df.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The feature to make lagged features from\n",
    "    col : str\n",
    "        The name of the feature\n",
    "    lags : list\n",
    "        The number of months to lag\n",
    "    merge_on : list\n",
    "        The columns to merge on\n",
    "    fillna : float\n",
    "        The value to fill the NaNs with\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lag_df : DataFrame\n",
    "        A DataFrame containing the lagged features\n",
    "    \"\"\"\n",
    "    \n",
    "    lag_df = df.loc[:, merge_on + [col]].copy()\n",
    "    \n",
    "    samples = lag_df.shape[0]\n",
    "    \n",
    "    for lag in lags:\n",
    "        print(f'Processing lag {lag}', end='\\r')\n",
    "        tmp_df = lag_df.copy()\n",
    "        tmp_df.loc[:, 'date_block_num'] = tmp_df.loc[:, 'date_block_num'] + lag\n",
    "        new_col = f'{col}_lag_{lag}'\n",
    "        tmp_df.rename({col: new_col}, axis=1, inplace=True)\n",
    "        tmp_df.drop_duplicates(inplace=True)\n",
    "        lag_df = pd.merge(lag_df, tmp_df.loc[:, [*merge_on, new_col]], how='left', on=merge_on)\n",
    "    \n",
    "    lag_df.fillna(fillna, inplace=True)\n",
    "    \n",
    "    # Drop the original feature\n",
    "    lag_df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    cur_samples = lag_df.shape[0]\n",
    "    if cur_samples > samples:\n",
    "        raise AssertionError(f'Sample size increased. Old: {samples}, new: {cur_samples}')\n",
    "    \n",
    "    return lag_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our strategy is to lag the features in the following way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Long lag** (i.e. by $1$, $2$, $3$, $6$, $9$ and $12$ months):\n",
    "These are the features which time trend is belived to be most important for prediction\n",
    "* Merge on: month, shop, item\n",
    "    * `month_shop_item_id_item_cnt_sum`\n",
    "* Merge on: month, shop\n",
    "    * `month_shop_item_cnt_sum`\n",
    "* Merge on: month, item\n",
    "    * `month_item_id_item_cnt_sum`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short lag** (i.e. by only $1$ month)\n",
    "These are the features which time trend is belived to be less important for prediction\n",
    "* Merge on: month, shop, item\n",
    "    * `month_shop_item_id_item_cnt_max`\n",
    "    * `month_shop_item_id_item_cnt_min`\n",
    "    * `month_shop_item_id_item_cnt_avg`\n",
    "* Merge on: month, shop, category\n",
    "    * `month_shop_item_cat_item_cnt_avg`\n",
    "* Merge on: month, shop\n",
    "    * `month_shop_item_cnt_avg`\n",
    "    * `month_shop_revenue_sum`\n",
    "* Merge on: month, item\n",
    "    * `month_item_id_item_cnt_max`\n",
    "    * `month_item_id_item_cnt_min`\n",
    "    * `month_item_id_item_cnt_avg`\n",
    "* Merge on: month, category\n",
    "    * `month_item_cat_item_cnt_avg`\n",
    "* Merge on: month\n",
    "    * `month_item_cnt_avg`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fillna strategy**:\n",
    "\n",
    "For the shop-item combinations we can safely fill the resulting `NaN`s with $0$ as the combination simply did not sell anything the previous `lag`-months.\n",
    "In fact, the same argument goes for all other mergin strategies as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long time lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_lags = [1, 2, 3, 6, 9, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date_block_num', 'item_id', 'shop_id']\n",
    "tmp = make_lagged(df=data_aggregate,\n",
    "                  col='month_shop_item_id_item_cnt_sum',\n",
    "                  lags=long_lags,\n",
    "                  merge_on=merge_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a minor sanity check: \n",
    "\n",
    "1. We select a non-frequent number of `'month_shop_item_id_item_cnt_sum'` in the zeroth month\n",
    "2. We loop throguh the item-shop combinations which has this number in a the zeroth month to check that it is present in all the lag months  \n",
    "3. We check that the lag values stays the same for the lagged values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'month_shop_item_id_item_cnt_sum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aggregate.loc[data_aggregate.loc[:, 'date_block_num'] == 0, col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_freq_number = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute force search\n",
    "\n",
    "search_through_df = data_aggregate.loc[(data_aggregate.loc[:, 'date_block_num'] == 0) &\n",
    "                                       (np.isclose(data_aggregate.loc[:, col], non_freq_number))]\n",
    "\n",
    "unique_item = search_through_df.loc[:, 'item_id']\n",
    "unique_shop = search_through_df.loc[:, 'shop_id']\n",
    "months = long_lags[:-2]\n",
    "\n",
    "item_id = None\n",
    "shop_id = None\n",
    "\n",
    "found = 0\n",
    "\n",
    "for cur_shop, cur_item in zip(unique_shop, unique_item):\n",
    "    for cur_month in months:\n",
    "        entries = data_aggregate.loc[(data_aggregate.loc[:, 'item_id'] == cur_item) &\n",
    "                                     (data_aggregate.loc[:, 'shop_id'] == cur_shop) &\n",
    "                                     (data_aggregate.loc[:, 'date_block_num'] == cur_month)]\n",
    "        if entries.shape[0] != 0:\n",
    "            found += 1\n",
    "        else:\n",
    "            found = 0\n",
    "            break\n",
    "            \n",
    "        if found == len(months):\n",
    "            # A combination of item_id and shop_id which is present in all months has been found \n",
    "            item_id = cur_item\n",
    "            shop_id = cur_shop\n",
    "    \n",
    "    if item_id is not None and shop_id is not None:\n",
    "        break\n",
    "\n",
    "if item_id is None and shop_id is None:\n",
    "    print('No entries matching the search was found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aggregate.loc[(data_aggregate.loc[:, 'item_id'] == item_id) &\n",
    "                   (data_aggregate.loc[:, 'shop_id'] == shop_id), ['item_id', 'shop_id', 'date_block_num', col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like this is a good item-shop combination to use for testing the lagging procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = set(long_lags).intersection(set(data_aggregate.loc[(data_aggregate.loc[:, 'item_id'] == item_id) &\n",
    "                                                            (data_aggregate.loc[:, 'shop_id'] == shop_id), \n",
    "                                                            'date_block_num']))\n",
    "\n",
    "for lag in months:\n",
    "    lagged_val = tmp.loc[(tmp.loc[:, 'date_block_num'] == lag) & \n",
    "                   (tmp.loc[:, 'shop_id'] == shop_id) &\n",
    "                   (tmp.loc[:, 'item_id'] == item_id),\n",
    "                   merge_on + [f'{col}_lag_{lag}']].drop_duplicates().\\\n",
    "                 loc[:, f'{col}_lag_{lag}']\n",
    "    if not np.isclose(non_freq_number, lagged_val):\n",
    "        raise AssertionError('Oh dear, something is wrong with the lag function...')\n",
    "\n",
    "print('Lag function looks OK!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aggregate = pd.merge(data_aggregate, tmp.drop_duplicates(), how='left', on=merge_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lagged_feature(df, col, merge_on, lags):\n",
    "    \"\"\"\n",
    "    Add a feature to the input data frame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The data frame to add the lagged features to\n",
    "    col : str\n",
    "        The feature to add make the lagged feature of\n",
    "    merge_on : list\n",
    "        List of features to merge on\n",
    "    lags : list\n",
    "        The lags\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        The data frame with added features\n",
    "    \"\"\"\n",
    "    tmp = make_lagged(df=df,\n",
    "                      col=col,\n",
    "                      lags=lags,\n",
    "                      merge_on=merge_on)\n",
    "    df = pd.merge(df, tmp.drop_duplicates(), how='left', on=merge_on)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date_block_num', 'shop_id']\n",
    "data_aggregate = add_lagged_feature(data_aggregate, 'month_shop_item_cnt_sum', merge_on, long_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date_block_num', 'item_id']\n",
    "data_aggregate = add_lagged_feature(data_aggregate, 'month_item_id_item_cnt_sum', merge_on, long_lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short time lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_lags = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date_block_num', 'shop_id', 'item_id']\n",
    "data_aggregate = add_lagged_feature(data_aggregate, 'month_shop_item_id_item_cnt_max', merge_on, short_lags)\n",
    "data_aggregate = add_lagged_feature(data_aggregate, 'month_shop_item_id_item_cnt_min', merge_on, short_lags)\n",
    "data_aggregate = add_lagged_feature(data_aggregate, 'month_shop_item_id_item_cnt_avg', merge_on, short_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date_block_num', 'shop_id', 'item_category_id']\n",
    "data_aggregate = add_lagged_feature(data_aggregate, 'month_shop_item_cat_item_cnt_avg', merge_on, short_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date_block_num', 'shop_id']\n",
    "data_aggregate = add_lagged_feature(data_aggregate, 'month_shop_item_cnt_avg', merge_on, short_lags)\n",
    "data_aggregate = add_lagged_feature(data_aggregate, 'month_shop_revenue_sum', merge_on, short_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date_block_num', 'item_id']\n",
    "data_aggregate = add_lagged_feature(data_aggregate, 'month_item_id_item_cnt_max', merge_on, short_lags)\n",
    "data_aggregate = add_lagged_feature(data_aggregate, 'month_item_id_item_cnt_min', merge_on, short_lags)\n",
    "data_aggregate = add_lagged_feature(data_aggregate, 'month_item_id_item_cnt_avg', merge_on, short_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date_block_num', 'item_category_id']\n",
    "data_aggregate = add_lagged_feature(data_aggregate, 'month_item_cat_item_cnt_avg', merge_on, short_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date_block_num']\n",
    "data_aggregate = add_lagged_feature(data_aggregate, 'month_item_cnt_avg', merge_on, short_lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verfiy that we did not introduced `NaN`s after the last "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_w_lag = [col for col in data_aggregate.columns if '_lag_' in col]\n",
    "if data_aggregate.loc[data_aggregate.loc[:, 'date_block_num'] > max(long_lags), cols_w_lag].isnull().any().any():\n",
    "    raise AssertionError('The monthly features contained something else than NaNs')\n",
    "    \n",
    "if data_aggregate.shape[0] > n_data_samples:\n",
    "    raise AssertionError(f'The set was expanded: '\n",
    "                         f'n_aggregates={n_aggregates} and n_data_aggregate={n_data_aggregate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del search_through_df\n",
    "del tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data now consist of the following columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(data_aggregate.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = Path('.').absolute().joinpath('generated_data')\n",
    "generated_data.mkdir(exist_ok=True)\n",
    "\n",
    "data_aggregate.to_hdf(generated_data.joinpath('data_aggregate.hdf'),\n",
    "                      key='data_aggregate')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
