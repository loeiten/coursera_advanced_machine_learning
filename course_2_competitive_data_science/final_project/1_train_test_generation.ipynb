{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here build the train, target and test set.\n",
    "We will store the datasets so they can nbe easily accessed when training and making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC\n",
    "\n",
    "* [Loading the data](#Loading-the-data)\n",
    "* [Cleaning of the dataset](#Cleaning-of-the-dataset)\n",
    "* [Adding date features](#Adding-date-features)\n",
    "* [Adding shop features](#Adding-shop-features)\n",
    "* [Adding item features](#Adding-item-features)\n",
    "* [Adding text features](#Adding-text-features)\n",
    "* [Adding leakage features](#Adding-leakage-features)\n",
    "* [Adding mean encoding](#Adding-mean-encoding)\n",
    "* [Feature dropping](#Feature-dropping)\n",
    "* [Normalization](#Normalization)\n",
    "* [Checking correlation](#Checking-correlation)\n",
    "* [Adding temporal history](#Adding-temporal-history)\n",
    "* [Storing the data frames](#Storing-the-data-frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import sklearn\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here generate the following features\n",
    "\n",
    "* Date features\n",
    "    * Year number\n",
    "    * Month number\n",
    "    * Days in month\n",
    "    * Quarter\n",
    "    * Holidays the current month\n",
    "    * Holidays the previous month\n",
    "    * Holidays the next month\n",
    "* Shop features \n",
    "    * Shop total revenue current month\n",
    "    * Shop total items sold current month\n",
    "    * Shop number of products\n",
    "* Item features \n",
    "    * Sum of item count aggregated by shop, item and month (target)\n",
    "    * Max of item count aggregated by shop, item and month \n",
    "    * Min of item count aggregated by shop, item and month \n",
    "    * Mean of item count aggregated by shop, item and month \n",
    "    * Sum of item count aggregated by item and month \n",
    "    * Max of item count aggregated by item and month \n",
    "    * Min of item count aggregated by item and month \n",
    "    * Mean of item count aggregated by item and month \n",
    "* Text features\n",
    "    * TF-IDF of item names, item category names and shops names\n",
    "    * Word count of cyrillic words in item names, item category names and shops names\n",
    "    * Word count of latin words in item names, item category names and shops names\n",
    "    * Total word count of words in item names, item category names and shops names\n",
    "* Leakage features\n",
    "    * Item occurences in the test set\n",
    "    * ID\n",
    "    * Row number\n",
    "* Mean encoding\n",
    "    * Expanding mean using the item id\n",
    "\n",
    "**NOTE**: As not all items are present in the training set we must be careful when we are filling out the missing values in the test set\n",
    "\n",
    "Other features which could have been generated\n",
    "\n",
    "* Geographical features of the shops\n",
    "    * Shop located in big city\n",
    "    * Nearest geographical neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on difference from the mean encoding assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the mean encoding assignment the training dataset was expanded by taking the outer product between the `item_id` and `shop_id` for all `date_block_num`.\n",
    "\n",
    "This has the atvantage of increased training data, and we could have employed that method here.\n",
    "\n",
    "By doing so we must take care that:\n",
    "\n",
    "* `date`, `item_price`, `item_cnt_day` wil have `NaN` and `NaT` values\n",
    "    * The `date` can be reconstructed to the first day in the month as we are not using the day level feature anywhere\n",
    "    * The `item_price` should have `NaN` values, and we should use `nanmin`, `nanmax` and `nanmean` when calculating the revenue\n",
    "    * The `item_cnt_day` should be set to `0` as there has not been any sales of this kind during the month under investigation.\n",
    "* For the leakage features should be made before expanding the training set\n",
    "    * `NaN`s in `row_number` and `ID` can be set to `-1`\n",
    "* ... possible other pitfalls must be taken into account\n",
    "\n",
    "The reason for not using this method here is mainly due to time contraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('.').absolute().joinpath('data')\n",
    "\n",
    "sales_train = pd.read_csv(data_dir.joinpath('sales_train.csv.gz'))\n",
    "sales_test = pd.read_csv(data_dir.joinpath('test.csv.gz'))\n",
    "items = pd.read_csv(data_dir.joinpath('items.csv'))\n",
    "item_categories = pd.read_csv(data_dir.joinpath('item_categories.csv'))\n",
    "shops = pd.read_csv(data_dir.joinpath('shops.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples = sales_train.shape[0]\n",
    "n_test_samples = sales_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast the dates to actual dates for easier manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train.loc[:, 'date'] = pd.to_datetime(sales_train.loc[:, 'date'], format='%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After investigating the item count per day outliners we saw that these may actual be correct (and not arising from typos etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we saw that the outliner in price could be fixed, by converting `'Radmin 3  - 522 лиц.'` to `'Radmin 3  - 1 лиц.'`.\n",
    "\n",
    "We do the conversion in the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values obtained from EDA\n",
    "item_count_522 = 522\n",
    "item_id_1 = 6065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_price = sales_train.loc[:, 'item_price'].max()\n",
    "high_price = sales_train.loc[sales_train.loc[:, 'item_price'] == max_price]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = high_price.index[0]\n",
    "\n",
    "sales_train.loc[index, 'item_id'] = item_id_1\n",
    "sales_train.loc[index, 'item_cnt_day'] = item_count_522\n",
    "sales_train.loc[index, 'item_price'] = max_price/item_count_522"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we saw that the datapoint for plastic bags had a high item count. As this is used to calculate the renevue below, we will not alter this item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen that that `item_cnt_day` are of floats, to speed up calculations, we transform them to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train.loc[:, 'item_cnt_day'] = sales_train.loc[:, 'item_cnt_day'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the base set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here use the trick from the ensembling exercise where we create an outer product of the `item_id` and `shop_id` present in each block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we are predicting for the `34`th month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test.loc[:, 'date_block_num'] = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "cols = ['date_block_num','shop_id','item_id']\n",
    "\n",
    "# The training part\n",
    "for block in sales_train.loc[:,'date_block_num'].unique():\n",
    "    tmp = sales_train.loc[sales_train.loc[:, 'date_block_num'] == block, cols]\n",
    "    # NOTE: Here we make an outer product of 'date_block_num','shop_id' and 'item_id'\n",
    "    data.append(np.array(list(product([block], tmp.loc[:,'shop_id'].unique(), tmp.loc[:,'item_id'].unique()))))\n",
    "\n",
    "# The test part\n",
    "for block in sales_test.loc[:,'date_block_num'].unique():\n",
    "    tmp = sales_test.loc[sales_test.loc[:, 'date_block_num'] == block, cols]\n",
    "    # NOTE: Here we make an outer product of 'date_block_num','shop_id' and 'item_id'\n",
    "    data.append(np.array(list(product([block], tmp.loc[:,'shop_id'].unique(), tmp.loc[:,'item_id'].unique()))))\n",
    "    \n",
    "# Make a sorted dataset of the list\n",
    "data = pd.DataFrame(np.vstack(data), columns=cols).sort_values(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sales_train.loc[:, ['date']]\n",
    "\n",
    "# Add one date for the predict date\n",
    "# NOTE: The relativedelta module takes care of problems with dates ending with 28, 30, 31\n",
    "next_month = dates['date'].max() + relativedelta(months=1)\n",
    "dates = pd.concat([dates, pd.DataFrame([next_month], columns=['date'])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates['date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the EDA that we found out that the last date in dataset was `2015-10-31`, this means we are going to predict for `2015-11`. \n",
    "\n",
    "Further, we note that only the year and month data is present in the test dataset, meaning that using information on the day level does not make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard date features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here add date features as seasonal trends are present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates.loc[:, 'year'] = dates.loc[:, 'date'].dt.year\n",
    "dates.loc[:, 'month'] = dates.loc[:, 'date'].dt.month\n",
    "dates.loc[:, 'days_in_month'] = dates.loc[:, 'date'].dt.days_in_month\n",
    "dates.loc[:, 'quarter'] = dates.loc[:, 'date'].dt.quarter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here generate the number of holidays in the previous month, the current month and the next month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_russian_holidays(year):\n",
    "    \"\"\"\n",
    "    Returns a Series of Russian holidays in a given year\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        The year to investigate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    holidays : Series\n",
    "        Series of the holidays on datetime64 format\n",
    "    \"\"\"\n",
    "    \n",
    "    url = f'https://www.timeanddate.com/holidays/russia/{year}'\n",
    "    html = requests.get(url).content\n",
    "    # A list is returned\n",
    "    table_df = pd.read_html(html)[0]\n",
    "    # Rename\n",
    "    table_df = table_df.rename(columns={'Date': 'date'})\n",
    "    holidays = pd.to_datetime(table_df['date'], format='%b %d')\n",
    "    \n",
    "    # Replace the year and cast to datetime\n",
    "    holidays = holidays.apply(lambda x: x.replace(year=year))\n",
    "\n",
    "    return holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year_months_len(df):\n",
    "    \"\"\"\n",
    "    Returns the number of entries grouped by year and month of the input data frame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        DataFrame with a column named 'date'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        The input DataFrame where the number of entries grouped by year and month\n",
    "        is appended to the column named 'year_month_count' \n",
    "    \"\"\"\n",
    "    \n",
    "    new_df = df.copy()\n",
    "    \n",
    "    new_df.loc[:, 'year'] = new_df.loc[:, 'date'].dt.year\n",
    "    new_df.loc[:, 'month'] = new_df.loc[:, 'date'].dt.month\n",
    "    \n",
    "    df.loc[:, 'year_month_count'] = new_df.groupby(['year', 'month'])['date'].transform(len)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We include 2012 to get the first prev_holiday_count later\n",
    "holiday_2012 = get_russian_holidays(2012).to_frame()\n",
    "holiday_2013 = get_russian_holidays(2013).to_frame()\n",
    "holiday_2014 = get_russian_holidays(2014).to_frame()\n",
    "holiday_2015 = get_russian_holidays(2015).to_frame()\n",
    "holidays = pd.concat([holiday_2012, holiday_2013, holiday_2014, holiday_2015])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_count = get_year_months_len(holidays).rename(columns={'year_month_count': 'holiday_count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now generate the previous month holidays count.\n",
    "We can get that by increasing the month by one (if the holiday count of February was 1 and the holiday count of March was 2, the holiday count of March will be 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_holiday_count = holiday_count.copy()\n",
    "prev_holiday_count.loc[:, 'date'] = prev_holiday_count.loc[:, 'date'] + pd.DateOffset(months=1)\n",
    "prev_holiday_count = prev_holiday_count.rename(columns={'holiday_count': 'prev_holiday_count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can find the next month holiday count by subtracting the months by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_holiday_count = holiday_count.copy()\n",
    "next_holiday_count.loc[:, 'date'] = next_holiday_count.loc[:, 'date'] + pd.DateOffset(months=-1)\n",
    "next_holiday_count = next_holiday_count.rename(columns={'holiday_count': 'next_holiday_count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the `date` and create `year` and `month` features we can merge on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: In order to merge the date data smoothly afterwards, we should drop the resulting duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_count.loc[:, 'year'] = holiday_count.loc[:, 'date'].dt.year\n",
    "holiday_count.loc[:, 'month'] = holiday_count.loc[:, 'date'].dt.month\n",
    "holiday_count.drop(['date'], axis=1, inplace=True)\n",
    "holiday_count.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_holiday_count.loc[:, 'year'] = prev_holiday_count.loc[:, 'date'].dt.year\n",
    "prev_holiday_count.loc[:, 'month'] = prev_holiday_count.loc[:, 'date'].dt.month\n",
    "prev_holiday_count.drop(['date'], axis=1, inplace=True)\n",
    "prev_holiday_count.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_holiday_count.loc[:, 'year'] = next_holiday_count.loc[:, 'date'].dt.year\n",
    "next_holiday_count.loc[:, 'month'] = next_holiday_count.loc[:, 'date'].dt.month\n",
    "next_holiday_count.drop(['date'], axis=1, inplace=True)\n",
    "next_holiday_count.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the previous, current and next holiday count into one frame.\n",
    "The resulting `NaN`s will be locations without vacations.\n",
    "We start by merging with `dates`, as this contains all relevant `year`-`month` combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = pd.merge(dates.loc[:, ['year', 'month']].drop_duplicates(),\n",
    "                    holiday_count, how='left', on=['year', 'month']).fillna(0)\n",
    "holidays = pd.merge(holidays, prev_holiday_count, how='left', on=['year', 'month']).fillna(0)\n",
    "holidays = pd.merge(holidays, next_holiday_count, how='left', on=['year', 'month']).fillna(0)\n",
    "\n",
    "# Re-shuffle the columns for better overview\n",
    "holidays = holidays.loc[:, ['year', 'month', 'prev_holiday_count', 'holiday_count', 'next_holiday_count']]\n",
    "\n",
    "# All columns can be integers\n",
    "holidays = holidays.astype(np.int32)\n",
    "\n",
    "# Sort by year and month for better overview\n",
    "holidays.sort_values(['year', 'month'], inplace=True)\n",
    "holidays.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del holiday_count\n",
    "del next_holiday_count\n",
    "del prev_holiday_count\n",
    "del holiday_2012\n",
    "del holiday_2013\n",
    "del holiday_2014\n",
    "del holiday_2015\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect that we did the correct thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding shop features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_drop_cols = ['item_price', 'item_cnt_day', 'item_category_id', 'revenue']\n",
    "shop = pd.merge(sales_train, items.loc[:, ['item_id', 'item_category_id']], how='left', on=['item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop.loc[:, 'revenue'] = shop.loc[:, 'item_price'] * shop.loc[:, 'item_cnt_day']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_sum = shop.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_sum.loc[:, 'month_shop_revenue_sum'] = \\\n",
    "    shop_sum.loc[:, ['date_block_num', 'shop_id', 'revenue']].\\\n",
    "    groupby(['date_block_num', 'shop_id'])['revenue'].transform(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_sum.loc[:, 'month_shop_item_item_cnt_sum'] = \\\n",
    "    shop_sum.loc[:, ['date_block_num', 'shop_id', 'item_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'shop_id', 'item_id'])['item_cnt_day'].transform(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_sum.loc[:, 'month_shop_item_cnt_sum'] = \\\n",
    "    shop_sum.loc[:, ['date_block_num', 'shop_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'shop_id'])['item_cnt_day'].transform(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_sum.drop(shop_drop_cols, axis=1, inplace=True)\n",
    "shop_sum.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_mean = shop.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_mean.loc[:, 'shop_revenue_avg'] = \\\n",
    "    shop_mean.loc[:, ['shop_id', 'revenue']].\\\n",
    "    groupby(['shop_id'])['revenue'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_mean.loc[:, 'month_shop_item_cnt_avg'] = \\\n",
    "    shop_mean.loc[:, ['date_block_num', 'shop_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'shop_id'])['item_cnt_day'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_mean.loc[:, 'month_shop_item_id_item_cnt_avg'] = \\\n",
    "    shop_mean.loc[:, ['date_block_num', 'shop_id', 'item_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'shop_id', 'item_id'])['item_cnt_day'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_mean.loc[:, 'month_shop_item_cat_item_cnt_avg'] = \\\n",
    "    shop_mean.loc[:, ['date_block_num', 'shop_id', 'item_category_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'shop_id', 'item_category_id'])['item_cnt_day'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_mean.drop(shop_drop_cols, axis=1, inplace=True)\n",
    "shop_mean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_other = shop.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_other.loc[:, 'shop_item_id_len'] = \\\n",
    "    shop_other.loc[:, ['shop_id', 'item_id']].\\\n",
    "    groupby('shop_id')['item_id'].transform(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_other.loc[:, 'month_shop_item_id_item_cnt_max'] = \\\n",
    "    shop_other.loc[:, ['date_block_num', 'shop_id', 'item_id', 'item_cnt_day']].\\\n",
    "    groupby(['shop_id', 'item_id', 'date_block_num'])['item_cnt_day'].transform(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_other.loc[:, 'month_shop_item_id_item_cnt_min'] = \\\n",
    "    shop_other.loc[:, ['date_block_num', 'shop_id', 'item_id', 'item_cnt_day']].\\\n",
    "    groupby(['shop_id', 'item_id', 'date_block_num'])['item_cnt_day'].transform(min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_other.drop(shop_drop_cols, axis=1, inplace=True)\n",
    "shop_other.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding non-shop features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shop_drop_cols = ['item_price', 'item_category_id', 'item_cnt_day']\n",
    "non_shop = pd.merge(sales_train, items.loc[:, ['item_id', 'item_category_id']], how='left', on=['item_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregates by item and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shop.loc[:, 'month_item_id_item_cnt_sum'] = \\\n",
    "    non_shop.loc[:, ['date_block_num', 'item_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'item_id'])['item_cnt_day'].transform(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shop.loc[:, 'month_item_id_item_cnt_avg'] = \\\n",
    "    non_shop.loc[:, ['date_block_num', 'item_category_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'item_category_id'])['item_cnt_day'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shop.loc[:, 'month_item_cat_item_cnt_avg'] = \\\n",
    "    non_shop.loc[:, ['date_block_num', 'item_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'item_id'])['item_cnt_day'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shop.loc[:, 'month_item_cnt_avg'] = \\\n",
    "    non_shop.loc[:, ['date_block_num', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num'])['item_cnt_day'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shop.loc[:, 'month_item_id_item_cnt_max'] = \\\n",
    "    non_shop.loc[:, ['date_block_num', 'item_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'item_id'])['item_cnt_day'].transform(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shop.loc[:, 'month_item_id_item_cnt_min'] = \\\n",
    "    non_shop.loc[:, ['date_block_num', 'item_id', 'item_cnt_day']].\\\n",
    "    groupby(['date_block_num', 'item_id'])['item_cnt_day'].transform(min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_shop.drop(non_shop_drop_cols, axis=1, inplace=True)\n",
    "non_shop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clipping the target value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this comptetition the range of the predicted item count sold should be in the range $[0, 20]$.\n",
    "This leaves us with two choices:\n",
    "\n",
    "1. Clipping before training\n",
    "2. Clipping after prediction\n",
    "\n",
    "The disatvantage of 1. is that this will give us inconsistencies with other features like the revenue.\n",
    "However, features like the aggregated revenue of a shop is expected to influence the sales of a shop, so it is not that critical that we reduce the correlation between these features.\n",
    "\n",
    "On the other hand, if we go for option 2., the range of values which the model is trying to learn from is broad. This can lead to low performance as the target space becomes broad and therefore sparse.\n",
    "\n",
    "Because of this we clip prior to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_mean.loc[:, 'month_shop_item_id_item_cnt_avg'].clip(0, 20, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge and clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_sum.drop_duplicates(inplace=True)\n",
    "shop_mean.drop_duplicates(inplace=True)\n",
    "shop_other.drop_duplicates(inplace=True)\n",
    "non_shop.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date', 'date_block_num', 'shop_id', 'item_id']\n",
    "\n",
    "aggregates = pd.merge(shop_sum, shop_mean, how='left', on=merge_on)\n",
    "aggregates = pd.merge(aggregates, shop_other, how='left', on=merge_on)\n",
    "aggregates = pd.merge(aggregates, non_shop, how='left', on=merge_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del shop\n",
    "del shop_sum\n",
    "del shop_mean\n",
    "del shop_other\n",
    "del non_shop\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding temporal history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a sequential problem, we would like to incorperate some time information into the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lagged(df, col, lag, fillna=0):\n",
    "    \"\"\"\n",
    "    Makes lagged features\n",
    "    \n",
    "    We make the lag this by adding the lag number to date_block_num\n",
    "    and merge the result on date_block_num of the corresponding month of the input df.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The feature to make lagged features from\n",
    "    col : str\n",
    "        The name of the feature\n",
    "    lag : list\n",
    "        The number of months to lag\n",
    "    fillna : float\n",
    "        The value to fill the NaNs with\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lag_df : DataFrame\n",
    "        A DataFrame containing the lagged features\n",
    "    \"\"\"\n",
    "    \n",
    "    merge_on = ['date_block_num', 'item_id', 'shop_id']\n",
    "    lag_df = df.loc[:, merge_on + ['date', col]].copy()\n",
    "    \n",
    "    samples = lag_df.shape[0]\n",
    "    \n",
    "    for l in lag:\n",
    "        print(f'Processing lag {l}')\n",
    "        tmp_df = df.loc[:, merge_on + [col]]\n",
    "        tmp_df.loc[:, 'date_block_num'] = tmp_df.loc[:, 'date_block_num'] + l\n",
    "        tmp_df.rename({col: f'{col}_lag_{l}'}, axis=1, inplace=True)\n",
    "        tmp_df.drop_duplicates(inplace=True)\n",
    "        lag_df = pd.merge(lag_df, tmp_df, how='left', on=merge_on)\n",
    "    \n",
    "    lag_df.fillna(fillna, inplace=True)\n",
    "    lag_df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    cur_samples = lag_df.shape[0]\n",
    "    if cur_samples != samples:\n",
    "        raise AssertionError(f'Sample sizes not matching. Old: {samples}, new: {cur_samples}')\n",
    "    \n",
    "    return lag_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end we will remove the features which results in `NaN`s, so we fill all merged features with `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long time lag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect that the long time dependency of the following features will be important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date_block_num', 'item_id', 'shop_id']\n",
    "lag = [1, 2, 3, 6, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'month_shop_item_id_item_cnt_avg'\n",
    "df = aggregates.loc[:, [col, 'date'] + merge_on]\n",
    "target_lag = make_lagged(df, col, lag)\n",
    "target_lag.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a small check that the algorithm is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg =\\\n",
    "    aggregates.loc[(aggregates.loc[:, 'date_block_num'] == 0) & \n",
    "                   (aggregates.loc[:, 'item_id'] == 2746) &\n",
    "                   (aggregates.loc[:, 'shop_id'] == 25),\n",
    "                   merge_on + ['month_shop_item_id_item_cnt_avg']].drop_duplicates().\\\n",
    "    loc[:, 'month_shop_item_id_item_cnt_avg']\n",
    "\n",
    "lag_1 =\\\n",
    "    target_lag.loc[(target_lag.loc[:, 'date_block_num'] == 1) & \n",
    "                   (target_lag.loc[:, 'item_id'] == 2746) &\n",
    "                   (target_lag.loc[:, 'shop_id'] == 25),\n",
    "                   merge_on + ['month_shop_item_id_item_cnt_avg_lag_1']].drop_duplicates().\\\n",
    "    loc[:, 'month_shop_item_id_item_cnt_avg_lag_1']\n",
    "\n",
    "lag_12 =\\\n",
    "    target_lag.loc[(target_lag.loc[:, 'date_block_num'] == 12) & \n",
    "                   (target_lag.loc[:, 'item_id'] == 2746) &\n",
    "                   (target_lag.loc[:, 'shop_id'] == 25),\n",
    "                   merge_on + ['month_shop_item_id_item_cnt_avg_lag_12']].drop_duplicates().\\\n",
    "    loc[:, 'month_shop_item_id_item_cnt_avg_lag_12']\n",
    "\n",
    "if (avg.values == lag_1.values).all() and (lag_1.values == lag_12.values).all():\n",
    "    print('Lag function looks OK!')\n",
    "else:\n",
    "    raise AssertionError('Oh dear, something is wrong with the lag function...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'month_item_id_item_cnt_avg'\n",
    "df = aggregates.loc[:, [col, 'date'] + merge_on]\n",
    "month_item = make_lagged(df, col, lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'month_shop_item_cnt_avg'\n",
    "df = aggregates.loc[:, [col, 'date'] + merge_on]\n",
    "month_shop = make_lagged(df, col, lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date', 'date_block_num', 'shop_id', 'item_id']\n",
    "\n",
    "aggregate_lag = pd.merge(aggregates, target_lag, how='left', on=merge_on)\n",
    "aggregate_lag = pd.merge(aggregate_lag, month_item, how='left', on=merge_on)\n",
    "aggregate_lag = pd.merge(aggregate_lag, month_shop, how='left', on=merge_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short time lag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect that the long time dependency of the following features will be less important than those above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['date_block_num', 'item_id', 'shop_id']\n",
    "lag = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_lags = ['month_shop_item_id_item_cnt_avg',\n",
    "             'month_item_id_item_cnt_avg',\n",
    "             'month_shop_item_cnt_avg']\n",
    "\n",
    "one_lag = [col for col in aggregate_lag if 'month' in col and\n",
    "           col not in done_lags and\n",
    "           '_lag_' not in col]\n",
    "\n",
    "for col in one_lag:\n",
    "    print(f'Processing {col}')\n",
    "    df = aggregates.loc[:, [col, 'date'] + merge_on]\n",
    "    lag_df = make_lagged(df, col, lag)\n",
    "    aggregate_lag = pd.merge(aggregate_lag, lag_df, how='left', on=['date']+merge_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del target_lag\n",
    "del month_item\n",
    "del month_shop\n",
    "del df\n",
    "del lag_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into the possibility that the names are correlated to the target, we add some text features as well. We split `item_name`, `shop_name` and `item_category_name` into cyrillic and latin words. We will stem these, and then combine them again before fitting a TF-IDF model to them.\n",
    "\n",
    "**NOTE**: The TF-IDF model does not care about the relative position of the words, so it is ok if the order is scrambled when recombining the words to sentences again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now like to stem the words (ideally we would like to lemmatize the words, but it looks like the lemmatization for non-english languages are not as readily available at the moment).\n",
    "\n",
    "**NOTE**: The stemmer casts to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_stemmer = nltk.stem.SnowballStemmer('russian')\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_cyrillic_latin(words):\n",
    "    \"\"\"\n",
    "    Separates the cyrillic and latin words\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    This function does not conserve word order\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    words : str\n",
    "        The string of words to be split\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    separated_words : str\n",
    "        The words separated by _SEP_\n",
    "        Cyrillic words are to the left of the separator, the latin to the right\n",
    "    \"\"\"\n",
    "    \n",
    "    words_split = words.split(' ')\n",
    "    cyrillic_words = list()\n",
    "    latin_words = list()\n",
    "    \n",
    "    for word in words_split:\n",
    "        # https://stackoverflow.com/questions/48255244/python-check-if-a-string-contains-cyrillic-characters\n",
    "        if re.search('[а-яА-Я]', word) is not None:\n",
    "            cyrillic_words.append(word)\n",
    "        else:\n",
    "            latin_words.append(word)\n",
    "    \n",
    "    cyrillic_words = ' '.join(cyrillic_words)\n",
    "    latin_words = ' '.join(latin_words)\n",
    "    \n",
    "    separated_words = f'{cyrillic_words}_SEP_{latin_words}'\n",
    "    \n",
    "    return separated_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_features(df, col, return_all=False):\n",
    "    \"\"\"\n",
    "    Returns a new DataFrame with added text features\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        The data frame to add the text features to\n",
    "    col : str\n",
    "        The column to obtain the text features from\n",
    "    return_all : bool\n",
    "        If True, intermediate columns will be returned\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_nlp : DataFrame\n",
    "        The data frame with the added text features\n",
    "        * {col}_clean - col column cleaned so that only alphabetical and numerical characters are present \n",
    "                        (only returned if return_all is True)\n",
    "        * cyrillic_latin - column where cyrillic and latin letters has been separated \n",
    "                           (only returned if return_all is True)\n",
    "        * cyrillic - column with only stemmed cyrillic words present (only returned if return_all is True)\n",
    "        * latin - column with only stemmed latin words present (only returned if return_all is True)\n",
    "        * {col}_nlp - combination of the cyrillic and latin column described above\n",
    "        * {col}_cyrillic_words - cyrillic word count\n",
    "        * {col}_latin_words - latin word count\n",
    "        * {col}_total_words - total word count\n",
    "    \"\"\"\n",
    "    \n",
    "    df_nlp = df.copy()\n",
    "    \n",
    "    # First we clean the text by removing non-alphabetical characters and non-numeric characters\n",
    "    \n",
    "    df_nlp.loc[:, f'{col}_clean'] = \\\n",
    "    df_nlp.loc[:, f'{col}'].apply(lambda s: re.sub('[^а-яА-Яa-zA-Z0-9 ]', ' ', s))\n",
    "\n",
    "    # Remove duplicated whitespaces\n",
    "    df_nlp.loc[:, f'{col}_clean'] = \\\n",
    "        df_nlp.loc[:, f'{col}_clean'].apply(lambda s: re.sub(' +',' ', s))\n",
    "    \n",
    "    df_nlp.loc[:, 'cyrillic_latin'] = df_nlp.loc[:, f'{col}_clean'].apply(separate_cyrillic_latin)\n",
    "    df_nlp.loc[:, 'cyrillic'] = df_nlp.loc[:, 'cyrillic_latin'].apply(lambda s: s.split('_SEP_')[0])\n",
    "    df_nlp.loc[:, 'latin'] = df_nlp.loc[:, 'cyrillic_latin'].apply(lambda s: s.split('_SEP_')[1])\n",
    "    \n",
    "    df_nlp.loc[:, 'cyrillic'] = df_nlp.loc[:, 'cyrillic'].apply(russian_stemmer.stem)\n",
    "    df_nlp.loc[:, 'latin'] = df_nlp.loc[:, 'latin'].apply(english_stemmer.stem)\n",
    "    \n",
    "    # Recombine words\n",
    "    df_nlp.loc[:, f'{col}_nlp'] = df_nlp.loc[:, 'cyrillic'].str[:] + ' ' + df_nlp.loc[:, 'latin'].str[:]\n",
    "    \n",
    "    # We add the word count of each type together with the total.\n",
    "    # The rationale for doing is\n",
    "    # 1. It's possible that product with complex names are not sold as much\n",
    "    # 2. In case there is a lot of English words in the product, it could be that it's less sellable in Russia\n",
    "    # 3. Possible other reasons not mentioned here\n",
    "    \n",
    "    df_nlp.loc[:, f'{col}_cyrillic_words'] = \\\n",
    "        df_nlp.loc[:, 'cyrillic'].apply(lambda s: len(s.split(' ')) if s != '' else 0)\n",
    "    df_nlp.loc[:, f'{col}_latin_words'] = \\\n",
    "        df_nlp.loc[:, 'latin'].apply(lambda s: len(s.split(' ')) if s != '' else 0)\n",
    "    \n",
    "    # NOTE: This is in fact an interaction feature\n",
    "    df_nlp.loc[:, f'{col}_total_words'] = \\\n",
    "        df_nlp.loc[:, f'{col}_cyrillic_words'] + df_nlp.loc[:, f'{col}_latin_words']\n",
    "    \n",
    "    if not return_all:\n",
    "        remove = [f'{col}_clean', 'cyrillic_latin', 'cyrillic', 'latin']\n",
    "        df_nlp.drop(remove, axis=1, inplace=True)\n",
    "    \n",
    "    return df_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_nlp = get_text_features(items, 'item_name')\n",
    "item_category_nlp = get_text_features(item_categories, 'item_category_name')\n",
    "shop_nlp = get_text_features(shops, 'shop_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many tokens we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_corpus = ' '.join(item_nlp.loc[:, 'item_name_nlp'].values)\n",
    "item_corpus_tokens = nltk.word_tokenize(item_corpus)\n",
    "print(f'Unique item_name_tokens {len(set(item_corpus_tokens))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_category_corpus = ' '.join(item_category_nlp.loc[:, 'item_category_name_nlp'].values)\n",
    "item_category_corpus_tokens = nltk.word_tokenize(item_category_corpus)\n",
    "print(f'Unique item_category_name_tokens {len(set(item_category_corpus_tokens))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_corpus = ' '.join(shop_nlp.loc[:, 'shop_name_nlp'].values)\n",
    "shop_corpus_tokens = nltk.word_tokenize(shop_corpus)\n",
    "print(f'Unique shop_name_tokens {len(set(shop_corpus_tokens))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should take care not to use all tokens as this may result in a [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). \n",
    "Let's see how the words are distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fd_item = nltk.FreqDist(item_corpus_tokens)\n",
    "fd_item.plot(samples, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fd_item_category = nltk.FreqDist(item_category_corpus_tokens)\n",
    "fd_item_category.plot(samples, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fd_shop = nltk.FreqDist(shop_corpus_tokens)\n",
    "fd_shop.plot(samples, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a couple of words constitutes the most of the corpuses. In other words we can expect a high information gain from the first couple of features and diminishing returns as we add more words. We will from graphical inspection try with max features $35$ for TF-IDF for the item corpus, $25$ for the item category corpus and $10$ for the shop corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = 35\n",
    "item_category_features = 25\n",
    "shop_features = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_item_vec = sklearn.feature_extraction.text.TfidfVectorizer(max_features=item_features)\n",
    "tf_idf_item = tf_idf_item_vec.fit_transform(item_nlp['item_name_nlp']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_item_category_vec = sklearn.feature_extraction.text.TfidfVectorizer(max_features=item_category_features)\n",
    "tf_idf_item_category = tf_idf_item_category_vec.fit_transform(item_category_nlp['item_category_name_nlp']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_shop_vec = sklearn.feature_extraction.text.TfidfVectorizer(max_features=shop_features)\n",
    "tf_idf_shop = tf_idf_shop_vec.fit_transform(shop_nlp['shop_name_nlp']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the TF-IDF results with the corresponding data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [f'item_tf_idf_{i}' for i in range(tf_idf_item.shape[1])]\n",
    "tf_idf_item_df = pd.DataFrame(tf_idf_item, columns=col_names)\n",
    "item_nlp = pd.concat([item_nlp, tf_idf_item_df], axis=1)\n",
    "item_nlp.drop(['item_name', 'item_category_id', 'item_name_nlp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [f'item_category_tf_idf_{i}' for i in range(tf_idf_item_category.shape[1])]\n",
    "tf_idf_item_category_df = pd.DataFrame(tf_idf_item_category, columns=col_names)\n",
    "item_category_nlp = pd.concat([item_category_nlp, tf_idf_item_category_df], axis=1)\n",
    "item_category_nlp.drop(['item_category_name', 'item_category_name_nlp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [f'shop_tf_idf_{i}' for i in range(tf_idf_shop.shape[1])]\n",
    "tf_idf_shop_df = pd.DataFrame(tf_idf_shop, columns=col_names)\n",
    "shop_nlp = pd.concat([shop_nlp, tf_idf_shop_df], axis=1)\n",
    "shop_nlp.drop(['shop_name', 'shop_name_nlp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding leakage features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leakage features are features where we use information about the test set.\n",
    "\n",
    "As both shop id and item id are features of the test set, and since these are not related to time, these are leakages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of ids in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_id_train = sales_train.loc[:, 'shop_id']\n",
    "shop_id_test = sales_test.loc[:, 'shop_id']\n",
    "shop_id_both = pd.concat([shop_id_train, shop_id_test], axis=0).to_frame()\n",
    "shop_id_both.loc[:, 'shop_id_count'] = shop_id_both.groupby('shop_id')['shop_id'].transform(len)\n",
    "\n",
    "# NOTE: Drop duplicated as we want to merge\n",
    "shop_id_both.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_train = sales_train.loc[:, 'item_id']\n",
    "item_id_test = sales_test.loc[:, 'item_id']\n",
    "item_id_both = pd.concat([item_id_train, item_id_test], axis=0).to_frame()\n",
    "item_id_both.loc[:, 'item_id_count'] = item_id_both.groupby('item_id')['item_id'].transform(len)\n",
    "\n",
    "# NOTE: Drop duplicated as we want to merge\n",
    "item_id_both.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiosity we check how these are distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "shop_id_both.loc[:, 'shop_id_count'].hist(ax=ax, bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_id_both.loc[:, 'shop_id_count'].value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the number of rows for each `shop_id` is well spread, and not clustering around a specific number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "item_id_both.loc[:, 'item_id_count'].hist(ax=ax, bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_both.loc[:, 'item_id_count'].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_both.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw from the EDA, we saw that the `ID` was highly correlated to the `shop_id`, so we include it here. Item and shops without an ID will be given `-1` (although we could probably construct a more appropriate `ID` feature if we checked the feature more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df = pd.merge(sales_train, sales_test, how='left', on=['shop_id', 'item_id'])\n",
    "id_df.loc[:,'ID'].fillna(-1, inplace=True)\n",
    "id_df.loc[:,'ID'] = merged_train.loc[:,'ID'].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the test set contains data after the train data, these will have a higher row number.\n",
    "Of course, we could be unlucky and have a test set which is shuffled with respect to the training set, but we nevertheless give it a shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_train = pd.DataFrame(list(range(len(sales_train.index))), columns=['row_nr'])\n",
    "row_test = pd.DataFrame(np.array(range(len(sales_test.index)))+row_train.iloc[-1].values[0], columns=['row_nr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding mean encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add a mean encoding feature. Generally the mean encoding is the black art of connecting information about the target to a feature without generating target leakage.\n",
    "\n",
    "**NOTE**: In order to get the correct, we must apply this after clipping the target to $[0,20]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'item_id'\n",
    "cumsum = merged_train.loc[:, ['target', col]].groupby(col)['target'].cumsum() - merged_train.loc[:, 'target']\n",
    "cumcnt = merged_train.loc[:, ['target', col]].groupby(col).cumcount()\n",
    "merged_train.loc[:, f'{col}_mean_enc_target_month'] = cumsum/cumcnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nans = merged_train.loc[merged_train.loc[:,f'{col}_mean_enc_target_month'].isnull(), f'{col}_mean_enc_target_month'].shape[0]\n",
    "n_train = merged_train.shape[0]\n",
    "frac = 100*(n_nans/n_train)\n",
    "\n",
    "print(f'{frac:.2f} % of the set contains NaNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_values = [1e-6, 0.01, 0.1, 1, 2]\n",
    "for close in close_values:\n",
    "    n_close = \\\n",
    "        merged_train.loc[(merged_train.loc[:,'target'] - close < merged_train.loc[:,f'{col}_mean_enc_target_month']) &\n",
    "                         (merged_train.loc[:,f'{col}_mean_enc_target_month'] < merged_train.loc[:,'target'] + close),\n",
    "                         f'{col}_mean_enc_target_month'].shape[0]\n",
    "    n_train = merged_train.shape[0]\n",
    "    frac = 100*(n_close/n_train)\n",
    "    \n",
    "    print(f'{frac:.2f} % of the set are +/- {close} from the actual target value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_coef = merged_train.loc[:,['target', f'{col}_mean_enc_target_month']].corr().values[0][1]\n",
    "print(f'The mean encoding has a correlation coefficent of {corr_coef:.2f} with the target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the missing values with the target mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = merged_train.loc[:,'target'].mean()\n",
    "print(f'The target mean is {target_mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train.loc[:, f'{col}_mean_enc_target_month'].fillna(target_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: We will not merge this with the test set as this is a feature which depends on the month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: We get a very high correlation in this feature, and we should consider to drop this feature when we are training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature dropping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now drop the features which are no longer needed.\n",
    "We will only keep those who are needed for the training.\n",
    "\n",
    "**NOTE**: Although tempting we should not get rid of `item_id` and `shop_id` even though we got `ID`, as these are used to identify the objects under investigation. Also note that these features are not ordinal, and it is possible that some very clever label encoding for these exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['date', \n",
    "             'item_category_id', \n",
    "             'item_category_name', \n",
    "             'item_cnt_day', \n",
    "             'item_name',\n",
    "             'item_price',\n",
    "             'revenue',\n",
    "             'shop_name']\n",
    "merged_train.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [col for col in drop_cols if col in merged_test.columns]\n",
    "merged_test.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: There is no need to [normalize the target](https://stats.stackexchange.com/questions/111467/is-it-necessary-to-scale-the-target-value-in-addition-to-scaling-features-for-re), however, we will use the target as a temporal feature (see below), which means that the temporal feature needs to be normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = pd.concat([merged_train, row_train], axis=1)\n",
    "merged_test = pd.concat([merged_test, row_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most item ids are present only a couple of times. This means that we have little amount of item level information for most items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = pd.merge(merged_train, item_id_both, how='left', on=['item_id'])\n",
    "merged_train = pd.merge(merged_train, shop_id_both, how='left', on=['shop_id'])\n",
    "\n",
    "merged_test = pd.merge(merged_test, item_id_both, how='left', on=['item_id'])\n",
    "merged_test = pd.merge(merged_test, shop_id_both, how='left', on=['shop_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge with the train set and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = pd.merge(merged_train, item_nlp, how='left', on=['item_id'])\n",
    "merged_train = pd.merge(merged_train, item_category_nlp, how='left', on=['item_category_id'])\n",
    "merged_train = pd.merge(merged_train, shop_nlp, how='left', on=['shop_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test = pd.merge(merged_test, item_nlp, how='left', on=['item_id'])\n",
    "merged_test = pd.merge(merged_test, item_category_nlp, how='left', on=['item_category_id'])\n",
    "merged_test = pd.merge(merged_test, shop_nlp, how='left', on=['shop_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train.loc[:, 'target_month'] = merged_train.loc[:, 'target'].copy()\n",
    "target = merged_train.loc[:, ['ID', 'item_id', 'shop_id', 'date_block_num', 'target']].copy()\n",
    "merged_train.drop('target', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Left-joining the data frames with the sales on the left conserves the training rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = pd.merge(sales_train, items, how='left', on=['item_id'])\n",
    "merged_train = pd.merge(merged_train, item_categories, how='left', on=['item_category_id'])\n",
    "merged_train = pd.merge(merged_train, shops, how='left', on=['shop_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: As we are making several `group_by` statements when generating the features, we should be careful and not combine the train and test set prior to the generation. Instead we should merge the generated features when possible, if not we should separately generate these features for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test = pd.merge(sales_test, items, how='left', on=['item_id'])\n",
    "merged_test = pd.merge(merged_test, item_categories, how='left', on=['item_category_id'])\n",
    "merged_test = pd.merge(merged_test, shops, how='left', on=['shop_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we already have the following encoding:\n",
    "\n",
    "- `shop_id` - `shop_name`\n",
    "- `item_id` - `item_name`\n",
    "- `item_category_id` - `item_category_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge `shop_n_products` to `merged_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_products = merged_train.loc[:, ['shop_n_products', 'shop_id', 'item_id']]\n",
    "shop_products.drop_duplicates(inplace=True)\n",
    "merged_test = pd.merge(merged_test, shop_products,\n",
    "                       how='left', on=['shop_id', 'item_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case a shop-item doens't exist in the train set, but in the test set we will set it to $-1$ as we do not know its value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test.loc[:, 'shop_n_products'].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall that tree-based models does not depend on the normalization, but non-tree-based models hugely depend on them. As we plan to use ensemble methods for the predictions we should normalize our data. Let's go through the normalization strategy for each of the features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use MaxMinScaler on these ordinal features\n",
    "* `ID`\n",
    "* `date_block_num`\n",
    "* `item_id` \n",
    "* `month`\n",
    "* `quarter`\n",
    "* `row_nr`\n",
    "* `shop_id`\n",
    "* `year`\n",
    "\n",
    "We will use StandardScaler on these numerical features in order to keep the distribution\n",
    "* `days_in_month`\n",
    "* `holiday_count`\n",
    "* `item_category_name_cyrillic_words`\n",
    "* `item_category_name_latin_words`\n",
    "* `item_category_name_total_words`\n",
    "* `item_id_count`\n",
    "* `item_id_mean_enc_target_month`\n",
    "* `item_count_high_month`\n",
    "* `item_count_low_month`\n",
    "* `item_count_mean_month`\n",
    "* `item_count_sum_month`\n",
    "* `item_name_cyrillic_words`\n",
    "* `item_name_latin_words`\n",
    "* `item_name_total_words`\n",
    "* `item_price`\n",
    "* `next_holiday_count`\n",
    "* `prev_holiday_count`\n",
    "* `revenue`\n",
    "* `shop_id_count`\n",
    "* `shop_item_cnt_month`\n",
    "* `shop_item_count_high_month`\n",
    "* `shop_item_count_low_month`\n",
    "* `shop_item_count_mean_month`\n",
    "* `shop_n_products`\n",
    "* `shop_name_cyrillic_words`\n",
    "* `shop_name_latin_words`\n",
    "* `shop_name_total_words`\n",
    "* `shop_revenue_month`\n",
    "* `target_month`\n",
    "\n",
    "These are already normalized\n",
    "* `item_category_tf_idf_*`\n",
    "* `item_tf_idf_*`\n",
    "* `shop_tf_idf_*`\n",
    "\n",
    "Due to time constraints, we will not use the Rank scaler or recast distribution (so that the become more Gaussian) with functions like `np.log`, although this could improve the quality of prediction from for example neural nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make copies of `ID`, `date_block_num`, `item_id` and `shop_id` so that we can operate with one scaled and one unscaled version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copies = ['ID', 'date_block_num', 'item_id', 'shop_id']\n",
    "\n",
    "for copy in copies:\n",
    "    merged_train.loc[:, f'{copy}_scaled'] = merged_train.loc[:, f'{copy}'].copy()\n",
    "    merged_test.loc[:, f'{copy}_scaled'] = merged_train.loc[:, f'{copy}'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: We postpone normalization of `date_block_num`, `item_id` and `shop_id` as we will use these to merge temporal features on later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_max_min = [\n",
    "    'ID_scaled',\n",
    "    'date_block_num_scaled',\n",
    "    'item_id_scaled',\n",
    "    'month',\n",
    "    'quarter',\n",
    "    'row_nr',\n",
    "    'shop_id_scaled',\n",
    "    'year']\n",
    "\n",
    "max_min_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "max_min_scaler.fit(merged_train.loc[:, features_max_min])\n",
    "merged_train.loc[:, features_max_min] = max_min_scaler.transform(merged_train.loc[:, features_max_min])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: We use the same scaler for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_max_min = [col for col in features_max_min if col in merged_test]\n",
    "merged_test.loc[:, features_max_min] = max_min_scaler.transform(merged_test.loc[:, features_max_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_standard_scaler = [\n",
    "    'days_in_month',\n",
    "    'holiday_count',\n",
    "    'item_category_name_cyrillic_words',\n",
    "    'item_category_name_latin_words',\n",
    "    'item_category_name_total_words',\n",
    "    'item_id_count',\n",
    "    'item_id_mean_enc_target_month',\n",
    "    'item_count_high_month',\n",
    "    'item_count_low_month',\n",
    "    'item_count_mean_month',\n",
    "    'item_count_sum_month',\n",
    "    'item_name_cyrillic_words',\n",
    "    'item_name_latin_words',\n",
    "    'item_name_total_words',\n",
    "    'next_holiday_count',\n",
    "    'prev_holiday_count',\n",
    "    'shop_id_count',\n",
    "    'shop_item_cnt_month',\n",
    "    'shop_item_count_high_month',\n",
    "    'shop_item_count_low_month',\n",
    "    'shop_item_count_mean_month',\n",
    "    'shop_n_products',\n",
    "    'shop_name_cyrillic_words',\n",
    "    'shop_name_latin_words',\n",
    "    'shop_name_total_words',\n",
    "    'shop_revenue_month',\n",
    "    'target_month'\n",
    "    ]\n",
    "\n",
    "standard_scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "test_features_standard_scaler = \\\n",
    "    [col for col in features_standard_scaler if col in merged_test]\n",
    "train_features_standard_scaler = set(features_standard_scaler) - set(test_features_standard_scaler)\n",
    "\n",
    "# Exlusive train features\n",
    "standard_scaler.fit(merged_train.loc[:, train_features_standard_scaler])\n",
    "merged_train.loc[:, train_features_standard_scaler] = \\\n",
    "    standard_scaler.transform(merged_train.loc[:, train_features_standard_scaler])\n",
    "\n",
    "# Train and test features\n",
    "standard_scaler.fit(merged_train.loc[:, test_features_standard_scaler])\n",
    "merged_train.loc[:, test_features_standard_scaler] = \\\n",
    "    standard_scaler.transform(merged_train.loc[:, test_features_standard_scaler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test.loc[:, test_features_standard_scaler] = \\\n",
    "    standard_scaler.transform(merged_test.loc[:, test_features_standard_scaler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiosity we check how the correlation map looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "corr = merged_train.corr()\n",
    "cax = ax.matshow(corr)\n",
    "fig.colorbar(cax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in the blanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the merging process has created a lot of NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_col = 'shop_revenue_month_lag_1'\n",
    "isnull = merged_train.loc[:, f'{check_col}'].isnull()\n",
    "\n",
    "isnull_pct = 100*merged_train.loc[isnull, f'{check_col}'].shape[0]/merged_train.shape[0]\n",
    "\n",
    "print(f'{isnull_pct:.2f} % of the rows of {check_col} in the training set bcontains NaNs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there may exist smarter ways to fill the NaN values (with the mean, median or a reconstructed values), we will simply fill them with $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train.fillna(0, inplace=True)\n",
    "merged_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: With this method we need to throw away `n_lag` months of our training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = set(merged_train.columns).symmetric_difference(set(merged_test.columns))\n",
    "\n",
    "if len(diff) != 0:\n",
    "    raise AssertionError(f'Difference in columns of merged_train and merged_test found:\\n{diff}')\n",
    "    \n",
    "if n_train_samples != merged_train.shape[0]:\n",
    "    raise AssertionError(f'Train samples introduced:\\nn_train_samples={n_train_samples}, '\n",
    "                         f'merged_train.shape[0]={merged_train.shape[0]}')\n",
    "\n",
    "if n_test_samples != merged_test.shape[0]:\n",
    "    raise AssertionError(f'Test samples introduced:\\nn_train_samples={n_test_samples}, '\n",
    "                         f'merged_test.shape[0]={merged_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = Path('.').absolute().joinpath('generated_data')\n",
    "generated_data.mkdir(exist_ok=True)\n",
    "\n",
    "merged_train.to_hdf(generated_data.joinpath('train.hdf'), key='train')\n",
    "target.to_hdf(generated_data.joinpath('target.hdf'), key='target')\n",
    "merged_test.to_hdf(generated_data.joinpath('test.hdf'), key='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
