{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will do the model selection and predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC\n",
    "\n",
    "* [Confusion warning](#Confusion-warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seeds\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Possible more fixes for non-determinism\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-366542480\n",
    "import os\n",
    "from keras import backend as k\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "\n",
    "# Limit operation to 1 thread for deterministic results.\n",
    "# NOTE: This will slow down the operation\n",
    "# session_conf = tf.ConfigProto(\n",
    "#     intra_op_parallelism_threads=1,\n",
    "#     inter_op_parallelism_threads=1)\n",
    "# sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "\n",
    "k.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = Path('.').absolute().joinpath('generated_data')\n",
    "\n",
    "train = pd.read_hdf(generated_data.joinpath('train.hdf'), key='train')\n",
    "target = pd.read_hdf(generated_data.joinpath('target.hdf'), key='target')\n",
    "test = pd.read_hdf(generated_data.joinpath('test.hdf'), key='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start we should do some rudimental model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `sample_submission.csv.gz` contains the constant prediction `0.5`, and gives the score `1.23646` against the kaggle site.\n",
    "\n",
    "Furthermore we know that the optimal prediction for a constant is a target mean (of the ground truth).\n",
    "Nevertheless, we can probe the leaderboard with target mean of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_prediction = test.loc[:, ['ID']]\n",
    "mean_prediction.loc[:, 'item_cnt_month'] = target.loc[:, 'target'].mean()\n",
    "\n",
    "# Set ID as index\n",
    "mean_prediction.set_index('ID', inplace=True)\n",
    "\n",
    "mean_prediction.to_csv('mean_prediction.csv')\n",
    "mean_prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean prediction gave a score of `4.48306`, which is worse than our initial submission.\n",
    "This means that on average, predictions with lower values are preferred over predicitions with higher values.\n",
    "\n",
    "We can in fact use this to probe the leaderboard. As we know that the constant target mean of the ground thruth gives the lowest score, we can check whether `0.5` is a minimum (at least of the public test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_04 = mean_prediction.copy()\n",
    "prediction_04.loc[:, 'item_cnt_month'] = 0.4\n",
    "prediction_04.to_csv('prediction_04.csv')\n",
    "prediction_04.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This improved the score to `1.22295`.\n",
    "\n",
    "We could continue to probe the leader board like this to find the minimum to get a direction on what our prediction mean should be close to. However, we must bear in mind that we are only probing the public part of the test set, so we must use this technique with care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here prepare the data for training and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove first months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the first months as the lagged values are effectively NaNs (which we replaced with $0$ in [1_train_test_generation.ipynb](1_train_test_generation.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_lag = max(set(int(col.split('_lag_')[-1]) for col in train.columns if '_lag_' in col))\n",
    "\n",
    "train = train.loc[train.loc[:, 'date_block_num'] >= highest_lag]\n",
    "target = target.loc[target.loc[:, 'date_block_num'] >= highest_lag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove superflous columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all(test.index == test.loc[:, 'ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the `ID` (which we only need in the test prediction) are stored in the index, so we might as well drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['ID', 'date_block_num', 'item_id', 'shop_id']\n",
    "\n",
    "train.drop(drop_cols, axis=1, inplace=True)\n",
    "target.drop(drop_cols, axis=1, inplace=True)\n",
    "test.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to save resources, we downcast the types (as they by default are loaded as double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    \"\"\"\n",
    "    Downcasts float64 to float32 and int64 to int32\n",
    "    \n",
    "    Paramters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The data frame to downcast\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        The downcasted date frame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df.columns if df.loc[:, c].dtype == 'float64']\n",
    "    int_cols = [c for c in df.columns if df.loc[:, c].dtype == 'int64']\n",
    "    \n",
    "    # Downcast\n",
    "    df.loc[:, float_cols] = df.loc[:, float_cols].astype(np.float32)\n",
    "    df.loc[:, int_cols] = df.loc[:, int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = downcast_dtypes(train)\n",
    "target = downcast_dtypes(target)\n",
    "test = downcast_dtypes(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the RSME is not available as a scorer out of the box, so we define it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Returns the root mean squared error of the predictions\n",
    "    \n",
    "    The root mean squared error is defined by:\n",
    "    $\\sqrt {\\frac {\\sum _{t=1}^{T}({\\hat {y}}_{t}-y_{t})^{2}}{T}}$\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ground_truth : array, shape (n_samples,)\n",
    "        The correct prediction\n",
    "    prediction : array, shape (n_samples,)\n",
    "        The predictions\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    rmse : float\n",
    "        The root mean squared error\n",
    "    \"\"\"\n",
    "    return np.sqrt(metrics.mean_squared_error(ground_truth, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_scorer = metrics.make_scorer(rmse, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The scorer in `GridSearchCV` can be utterly confusing. \n",
    "\n",
    "The scores returned by `GridSearchCV` are negative for scores as `GridSearchCV` by convention tries to maximize its score. This means that loss functions like MSE have to be negated.\n",
    "\n",
    "See [here](https://stackoverflow.com/questions/21050110/sklearn-gridsearchcv-with-pipeline)\n",
    "and [here](https://stackoverflow.com/questions/21443865/scikit-learn-cross-validation-negative-values-with-mean-squared-error).\n",
    "\n",
    "For clarity let's run an experiment.\n",
    "We will fit a linear regression classifier on data that intersects (0, 1) rather than origo.\n",
    "Thus we know that including the intercept is better than not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_train = pd.DataFrame(np.array(range(100)))\n",
    "example_target = pd.DataFrame(np.array(range(1, 101)))\n",
    "\n",
    "parameters = {'fit_intercept': (True, False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_scorer_greater = metrics.make_scorer(rmse, greater_is_better=True)\n",
    "grid_lin_greater = model_selection.GridSearchCV(LinearRegression(), \n",
    "                                                parameters,\n",
    "                                                scoring=rmse_scorer_greater,\n",
    "                                                return_train_score=False)\n",
    "grid_lin_greater.fit(example_train, example_target)\n",
    "greater_best_model = grid_lin_greater.best_estimator_\n",
    "greater_best_score = grid_lin_greater.best_score_\n",
    "greater_mean_score = grid_lin_greater.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_scorer_lesser = metrics.make_scorer(rmse, greater_is_better=False)\n",
    "grid_lin_lesser = model_selection.GridSearchCV(LinearRegression(), \n",
    "                                               parameters,\n",
    "                                               scoring=rmse_scorer_lesser,\n",
    "                                               return_train_score=False)\n",
    "grid_lin_lesser.fit(example_train, example_target)\n",
    "lesser_best_model = grid_lin_lesser.best_estimator_\n",
    "lesser_best_score = grid_lin_lesser.best_score_\n",
    "lesser_mean_score = grid_lin_lesser.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Greater is best returns best a score of {greater_best_score:.2f} '\n",
    "      f'of {greater_mean_score} '\n",
    "      f'with the model\\n{greater_best_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Lesser is best returns best a score of {lesser_best_score:.2f} '\n",
    "      f'of {lesser_mean_score} '\n",
    "      f'with the model\\n{lesser_best_model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to predict for the next month, we know that train-test is split by time (we would like to predict for month $34$).\n",
    "\n",
    "In addition, from [0_EDA.ipynb](0_EDA.ipynb), we saw that different band of item ids were removed (i.e. non-random row numbers were removed in the training set). \n",
    "\n",
    "As a rule of thumb we should mimic the validation in the similar manner. The time component is fairly straigth forward. The question is whether it makes sense to take out bands of item id in addition. This is of course testable, and due to time constraints we will just split by time here.\n",
    "\n",
    "**NOTE**: Training takes quite some time with extensive grid search with using this dataset. It would be optimal to have several splits, but as we a high number of samples we will only use one split here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_generator = model_selection.TimeSeriesSplit(n_splits=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_skl(name,\n",
    "                       estimator,\n",
    "                       parameters,\n",
    "                       train,\n",
    "                       target,\n",
    "                       scorer,\n",
    "                       cv_generator,\n",
    "                       save_dir,\n",
    "                       overwrite=False\n",
    "                      ):\n",
    "    \"\"\"\n",
    "    Performs cross validation on a scikit learn estimator.\n",
    "    \n",
    "    The function will search for saved models and load them unless overwrite is True\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        Name to add to the model name for saving and loading\n",
    "    estimator : estimator object\n",
    "        The estimator to perform the cross validation on\n",
    "    parameters : dict\n",
    "        Parameters to tune\n",
    "    train : array-like\n",
    "        The training set\n",
    "    target : array-like\n",
    "        The target\n",
    "    scorer : scorer object\n",
    "        The scorer to use in the cross validation\n",
    "    cv_generator : cv-generator object\n",
    "        An object to use for the train-validation split in the cross validation\n",
    "    save_dir : Path or str\n",
    "        Directory to save the model to\n",
    "    overwrite : bool\n",
    "        Will overwrite existing pickled models, overrides the new_model parameter\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    best_estimator : estimator object\n",
    "        The best estimator found by the search\n",
    "    train_score : array\n",
    "        The training score\n",
    "    validation_score : array\n",
    "        The validation score\n",
    "    \"\"\"\n",
    "    \n",
    "    model_name = str(estimator.__class__).split('.')[-1][:-2]\n",
    "    file_path = Path(save_dir).joinpath(f'{model_name}_{name}.pkl')\n",
    "    \n",
    "    if file_path.is_file() and not overwrite:  \n",
    "        with file_path.open('rb') as f:\n",
    "            model_grid = pickle.load(f)\n",
    "        print(f'Loaded fitted model grid from {file_path}')\n",
    "    else:\n",
    "        model_grid = model_selection.GridSearchCV(estimator, \n",
    "                                                  parameters,\n",
    "                                                  scoring=rmse_scorer,\n",
    "                                                  cv=cv_generator,\n",
    "                                                  verbose=3,\n",
    "                                                  return_train_score=True)\n",
    "        model_grid.fit(train, target)\n",
    "        \n",
    "        with file_path.open('wb') as f:\n",
    "            pickle.dump(model_grid, f, pickle.HIGHEST_PROTOCOL)\n",
    "        print(f'Saved fitted model grid to {file_path}')\n",
    "    \n",
    "    best_estimator = model_grid.best_estimator_\n",
    "    \n",
    "    train_score = model_grid.cv_results_[\"mean_train_score\"]\n",
    "    validation_score = model_grid.cv_results_[\"mean_test_score\"]\n",
    "    \n",
    "    return best_estimator, train_score, validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_validation(train_scores, validation_scores, parameter):\n",
    "    \"\"\"\n",
    "    Plots the training and validation curve as a function of the parameter\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_scores : array-like\n",
    "        The scores obtained from the training set\n",
    "    validation_scores : array-like\n",
    "        The scores obtained from the validation set\n",
    "    parameters : dict\n",
    "        Dictionary of the tuned parameter on the form\n",
    "        >>> {parameter: np.array}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    key = parameter.keys()\n",
    "    parameter_vals = parameter[key]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "        \n",
    "    ax.plot(parameter_vals, train_scores, label='Train')\n",
    "    ax.plot(parameter_vals, validation_scores, label='Validation')\n",
    "    ax.set_xlabel(key)\n",
    "    ax.set_ylabel('Error')\n",
    "    ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no real hyperparameters to tune in linear regression (other than choosing wheter we should include the intersect or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg, lin_train_score, lin_validation_score = \\\n",
    "    cross_validate_skl('fit_intercept',\n",
    "                       LinearRegression(), \n",
    "                       {'fit_intercept': (True,)}, \n",
    "                       train, \n",
    "                       target, \n",
    "                       rmse_scorer, \n",
    "                       cv_generator, \n",
    "                       generated_data,overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the [Confusion warning](#Confusion-warning) section, we need to negate the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train score: {-lin_train_score[0]:.3f}')\n",
    "print(f'Validation score: {-lin_validation_score[0]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the RMSE is quite high, it could be that it can add some information to a stacked data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_reg, knn_train_score, knn_validation_score = \\\n",
    "    cross_validate_skl('k_1-4',\n",
    "                       KNeighborsRegressor(), \n",
    "                       {'n_neighbors': (1, 2, 3, 4), 'n_jobs': (-1,)}, \n",
    "                       train, \n",
    "                       target, \n",
    "                       rmse_scorer, \n",
    "                       cv_generator, \n",
    "                       generated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the other estimators, the `xgboost` estimator has several knobs to turn which can be used to find the optimal estimator.\n",
    "\n",
    "To start with, we have:\n",
    "\n",
    "Better fitting (increase for reducing underfit)\n",
    "* max_depth\n",
    "* subsample\n",
    "* colsample_bytree\n",
    "* colsample_bylevel\n",
    "* eta \n",
    "* num_round\n",
    "\n",
    "Impeeds fitting (increase for reducing overfitting)\n",
    "* min_child_weight\n",
    "* lambda\n",
    "* alpha\n",
    "\n",
    "We will start with the `max_depth` parameter to investigate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
    "\n",
    "params = {'max_depth': (3, 4, 5),\n",
    "          'n_jobs': (-1,),\n",
    "          'random_state': (seed,),\n",
    "         }\n",
    "\n",
    "xg_reg, xg_train_score, xg_validation_score = \\\n",
    "    cross_validate_skl('depth_3-5',\n",
    "                       XGBRegressor(), \n",
    "                       params, \n",
    "                       train, \n",
    "                       target, \n",
    "                       rmse_scorer, \n",
    "                       cv_generator, \n",
    "                       generated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: We also make a costum RMSE for keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_keras(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: RNNs would probably be the best fit for this task, we will for simplicity use plain old multilayer perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_dim, optimizer, hidden_layers=1, nodes=32, dropout=0):\n",
    "    \"\"\"\n",
    "    Returns a keras model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        The input dimension\n",
    "    hidden_layers : int\n",
    "        The number of hidden layers\n",
    "    optimizer : str\n",
    "        The optimizer to use\n",
    "    nodes : int or array-like, shape (hidden_layers)\n",
    "        Nodes for all the layers.\n",
    "        If array-like, each element corresponds to the nodes in the hidden layer\n",
    "        If int, all hidden layers will have the same number of nodes\n",
    "    dropout : float or array-like, shape (hidden_layers)\n",
    "        Dropout for all the layers.\n",
    "        If array-like, each element corresponds to the dropout values after each hidden layer\n",
    "        If int, all hidden layers will have the same dropout value\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(nodes) == int:\n",
    "        nodes = [nodes] * hidden_layers\n",
    "    if type(dropout) == float or type(dropout) == int:\n",
    "        dropout = [dropout] * hidden_layers\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(nodes[0], input_dim=input_dim))\n",
    "    \n",
    "    if len(nodes) > 1:\n",
    "        model.add(Activation('relu'))\n",
    "    \n",
    "    for node, drop in zip(nodes[1:], dropout[:-1]):\n",
    "        model.add(Dropout(drop))\n",
    "        model.add(Dense(node))\n",
    "        \n",
    "        if node != nodes[-1]:\n",
    "            model.add(Activation('relu'))\n",
    "\n",
    "    # Add the final layer\n",
    "    model.add(Dropout(dropout[-1]))\n",
    "    model.add(Dense(1))\n",
    "    # NOTE: We use identity as we are dealing with a regression problem\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    model.compile(loss=rmse_keras,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[rmse_keras])\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the neural networks, it makes sense to investigate\n",
    "\n",
    "Better fitting (increase for reducing underfit)\n",
    "* Number of neurons per layer\n",
    "* Number of layers\n",
    "* Adam/Adadelta/Adagrad/... (observed to lead to more overfitting)\n",
    "* Batch size\n",
    "\n",
    "Impeeds fitting (increase for reducing overfitting)\n",
    "* L2/L1 for weights\n",
    "* Dropout/Dropconnect\n",
    "* Static dropconnect\n",
    "\n",
    "We start by optimizing one layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = KerasRegressor(build_fn=build_mlp, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'input_dim': (train.shape[1],),\n",
    "          'optimizer': ('adadelta',),\n",
    "          'hidden_layers' : (1,),\n",
    "          'nodes': (16, 32, 64),\n",
    "          'dropout': (0.3,),\n",
    "          'batch_size': (32,),\n",
    "          'epochs': (10,)\n",
    "         }\n",
    "\n",
    "map_reg, map_train_score, map_validation_score = \\\n",
    "    cross_validate_skl('hl_1_n_16-64_do_03',\n",
    "                       mlp_model, \n",
    "                       params, \n",
    "                       train, \n",
    "                       target, \n",
    "                       rmse_scorer, \n",
    "                       cv_generator, \n",
    "                       generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'input_dim': (train.shape[1],),\n",
    "          'optimizer': ('adadelta',),\n",
    "          'hidden_layers' : (1,),\n",
    "          'nodes': (2,),\n",
    "          'dropout': (0,),\n",
    "          'batch_size': (32,),\n",
    "          'epochs': (1,)\n",
    "         }\n",
    "\n",
    "map_reg, map_train_score, map_validation_score = \\\n",
    "    cross_validate_skl('hl_1_n_16-64_do_03',\n",
    "                       mlp_model, \n",
    "                       params, \n",
    "                       train, \n",
    "                       target, \n",
    "                       rmse_scorer, \n",
    "                       cv_generator, \n",
    "                       generated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_pred = lin_reg.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should be a different notebook?\n",
    "\n",
    "If time: Submit target mean and see what score we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If mismatch between submission score and local validation score, check if:\n",
    "    \n",
    "* Too little data in public leader board\n",
    "* We overfitted\n",
    "* Chosen the correct splitting strategy\n",
    "* Train/test comes from different distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anne Gunn 8-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the submission sample, report score\n",
    "\n",
    "Report score of optimal value (for RSME this is the target mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clarity\n",
    "\n",
    "- The clear step-by-step instruction on how to produce the final submit file is provided\n",
    "- Code has comments where it is needed and meaningful function names\n",
    "\n",
    "\n",
    "\n",
    "Validation\n",
    "\n",
    "- Type of public/private split is identified (leaderboard probing)\n",
    "\n",
    "Data leakages\n",
    "\n",
    "- Data is investigated for data leakages and investigation process is described\n",
    "- Found data leakages are utilized\n",
    "\n",
    "Metrics optimization\n",
    "\n",
    "- Correct metric is optimized\n",
    "\n",
    "Advanced Features I: mean encodings\n",
    "\n",
    "- Mean-encoding is applied\n",
    "- Mean-encoding is set up correctly, i.e. KFold or expanding scheme are utilized correctly\n",
    "\n",
    "Advanced Features II\n",
    "\n",
    "- At least one feature from this topic is introduced\n",
    "\n",
    "Hyperparameter tuning\n",
    "\n",
    "- Parameters of models are roughly optimal\n",
    "\n",
    "Ensembles\n",
    "\n",
    "- Ensembling is utilized (linear combination counts)\n",
    "- Validation with ensembling scheme is set up correctly, i.e. KFold or Holdout is utilized\n",
    "- Models from different classes are utilized (at least two from the following: KNN, linear models, RF, GBDT, NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clarity\n",
    "\n",
    "- The clear step-by-step instruction on how to produce the final submit file is provided\n",
    "- Code has comments where it is needed and meaningful function names\n",
    "\n",
    "Feature preprocessing and generation with respect to models\n",
    "\n",
    "- Several simple features are generated\n",
    "- For non-tree-based models preprocessing is used or the absence of it is explained\n",
    "- Feature extraction from text and images\n",
    "\n",
    "Features from text are extracted\n",
    "\n",
    "- Special preprocessings for text are utilized (TF-IDF, stemming, levenshtening...)\n",
    "\n",
    "EDA\n",
    "\n",
    "- Several interesting observations about data are discovered and explained\n",
    "- Target distribution is visualized, time trend is assessed\n",
    "\n",
    "Validation\n",
    "\n",
    "- Type of train/test split is identified and used for validation\n",
    "- Type of public/private split is identified\n",
    "\n",
    "Data leakages\n",
    "\n",
    "- Data is investigated for data leakages and investigation process is described\n",
    "- Found data leakages are utilized\n",
    "\n",
    "Metrics optimization\n",
    "\n",
    "- Correct metric is optimized\n",
    "\n",
    "Advanced Features I: mean encodings\n",
    "\n",
    "- Mean-encoding is applied\n",
    "- Mean-encoding is set up correctly, i.e. KFold or expanding scheme are utilized correctly\n",
    "\n",
    "Advanced Features II\n",
    "\n",
    "- At least one feature from this topic is introduced\n",
    "\n",
    "Hyperparameter tuning\n",
    "\n",
    "- Parameters of models are roughly optimal\n",
    "\n",
    "Ensembles\n",
    "\n",
    "- Ensembling is utilized (linear combination counts)\n",
    "- Validation with ensembling scheme is set up correctly, i.e. KFold or Holdout is utilized\n",
    "- Models from different classes are utilized (at least two from the following: KNN, linear models, RF, GBDT, NN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
