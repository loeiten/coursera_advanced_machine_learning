{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial keypoints detection\n",
    "\n",
    "In this task you will create facial keypoint detector based on CNN regressor.\n",
    "\n",
    "\n",
    "![title](example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script `get_data.py` unpacks data — images and labelled points. 6000 images are located in `images` folder and keypoint coordinates are in `gt.csv` file. Run the cell below to unpack data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_data import unpack\n",
    "from pathlib import Path\n",
    "cur_dir = Path('.').absolute()\n",
    "data_dir = cur_dir.joinpath('data')\n",
    "if not data_dir.is_dir():\n",
    "    unpack('data.tar.xz', cur_dir)\n",
    "else:\n",
    "    print('\"data\" is already unpacked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have to read `gt.csv` file and images from `images` dir. File `gt.csv` contains header and ground truth points for every image in `images` folder. It has 29 columns. First column is a filename and next 28 columns are `x` and `y` coordinates for 14 facepoints. We will make following preprocessing:\n",
    "1. Scale all images to resolution $100 \\times 100$ pixels.\n",
    "2. Scale all coordinates to range $[-0.5; 0.5]$. To obtain that, divide all x's by width (or number of columns) of image, and divide all y's by height (or number of rows) of image and subtract 0.5 from all values.\n",
    "\n",
    "Function `load_imgs_and_keypoint` should return a tuple of two numpy arrays: `imgs` of shape `(N, 100, 100, 3)`, where `N` is the number of images and `points` of shape `(N, 28)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.color import gray2rgb\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imgs(dirname):\n",
    "    \"\"\"\n",
    "    Obtain images from dirname\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dirname : Path or str\n",
    "        The directory to obtain the images from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    imgs : np.array, shape (N, 100, 100, 3)\n",
    "        The N images in the dirname directory\n",
    "    orig_width : np.array, shape (N,)\n",
    "        The original widths of the images\n",
    "    orig_height : np.array, shape (N,)\n",
    "        The original heights of the images\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sorting to let the filename correspond with points\n",
    "    img_paths = sorted(dirname.glob('**/*.jpg'))\n",
    "    n_imgs = len(img_paths)\n",
    "    size = 100\n",
    "    \n",
    "    # Define the shape of imgs\n",
    "    imgs = np.zeros((n_imgs, size, size, 3))\n",
    "    orig_width = np.zeros(n_imgs)\n",
    "    orig_height = np.zeros(n_imgs)\n",
    "    \n",
    "    for nr, img_path in enumerate(tqdm_notebook(img_paths)):\n",
    "        img = imread(img_path)\n",
    "        # Ensure 3 channels\n",
    "        if len(img.shape) == 2:\n",
    "            img = gray2rgb(img)\n",
    "            \n",
    "        # Store width and height\n",
    "        orig_width[nr] = img.shape[0]\n",
    "        orig_height[nr] = img.shape[1]\n",
    "        \n",
    "        # Resize and store\n",
    "        img = resize(img, (size, size), mode='reflect', anti_aliasing=True)\n",
    "        imgs[nr, :, :, :] = img\n",
    "        \n",
    "    return imgs, orig_width, orig_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points(dirname, orig_width, orig_height):\n",
    "    \"\"\"\n",
    "    Obtain points from dirname\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dirname : Path or str\n",
    "        The directory to obtain the points from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    points : np.array, shape (N, 28)\n",
    "        Keypoints belonging to corresponding images\n",
    "    orig_width : np.array, shape (N,)\n",
    "        The original widths of the images\n",
    "    orig_height : np.array, shape (N,)\n",
    "        The original heights of the images\n",
    "    \"\"\"\n",
    "    \n",
    "    csv_path = data_dir.joinpath('gt.csv')\n",
    "    points_df = pd.read_csv(csv_path)\n",
    "    # NOTE: The filename is sorted in the same manner as imgs are\n",
    "    # NOTE: Casting to float to use the /= operator\n",
    "    points = points_df.loc[:, [col for col in points_df.columns if col != 'filename']].values.astype(float)\n",
    "\n",
    "    # Normalize\n",
    "    for i in tqdm_notebook(range(points.shape[0])):\n",
    "        # NOTE: The columns are arranged like the following\n",
    "        #       x1 y1 x2 x2 ... x14 y14\n",
    "        # Normalize width (the xs) by taking every second column\n",
    "        points[i, ::2] /= orig_width[i]\n",
    "        # Normalize height (the ys) by taking every second column starting from 1\n",
    "        points[i, 1::2] /= orig_height[i]\n",
    "    \n",
    "    # Scale to [-0.5, 0.5]\n",
    "    points -= 0.5\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imgs_and_keypoints(dirname=cur_dir.joinpath('data')):\n",
    "    \"\"\"\n",
    "    Loads the images and keypoints\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dirname : Path or str\n",
    "        The directory to obtain the data from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    imgs : np.array, shape (N, 100, 100, 3)\n",
    "        The N images in the dirname directory\n",
    "    points : np.array, shape (N, 28)\n",
    "        Keypoints belonging to imgs\n",
    "    \"\"\"\n",
    "    \n",
    "    imgs, orig_width, orig_height = get_imgs(dirname)\n",
    "    points = get_points(dirname, orig_width, orig_height)\n",
    "\n",
    "    return imgs, points\n",
    "\n",
    "imgs, points = load_imgs_and_keypoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Max {points.max():.2f} found at image {np.where(np.isclose(points, points.max()))[0][0]}')\n",
    "print(f'Min {points.min():.2f} found at image {np.where(np.isclose(points, points.min()))[0][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: By inspecting the images with the max and the min, we indeed find that these points are outside the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of output\n",
    "%matplotlib inline\n",
    "from skimage.io import imshow\n",
    "imshow(imgs[0])\n",
    "points[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare a function to visualize points on image. Such function obtains two arguments: an image and a vector of points' coordinates and draws points on image (just like first image in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "def visualize_points(img, points):\n",
    "    \"\"\"\n",
    "    Visualizes the points on the image\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.array, shape (cols, rows)\n",
    "        The image\n",
    "    points : np.array, shape (28)\n",
    "        The points given like x1 y1 x2 y2 ... x14 y14\n",
    "    \"\"\"\n",
    "    # Make point pairs\n",
    "    point_pairs = list(zip(points[::2], points[1::2]))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    # Plot the image\n",
    "    ax.imshow(img)\n",
    "    # Plot the points\n",
    "    for x, y in point_pairs:\n",
    "        # Backtransform\n",
    "        x = (x+0.5)*100\n",
    "        y = (y+0.5)*100\n",
    "        # Plot\n",
    "        circle = plt.Circle((x, y), radius=1, color='r')\n",
    "        ax.add_artist(circle)\n",
    "    \n",
    "visualize_points(imgs[1], points[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/val split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to obtain train/validation split for training neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "imgs_train, imgs_val, points_train, points_val = train_test_split(imgs, points, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better training we will use simple data augmentation — flipping an image and points. Implement function flip_img which flips an image and its' points. Make sure that points are flipped correctly! For instance, points on right eye now should be points on left eye (i.e. you have to mirror coordinates and swap corresponding points on the left and right sides of the face). Visualize an example of original and flipped image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_img(img, points):\n",
    "    \"\"\"\n",
    "    Flips and image and its points\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.array, shape (100, 100)\n",
    "        The image to flip\n",
    "    points : np.array, shape (28)\n",
    "        The points given like x1 y1 x2 y2 ... x14 y14\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    f_img : np.array, shape (100, 100)\n",
    "        The flipped image\n",
    "    f_points : np.array, shape (100, 100)\n",
    "        The flipped points given like x1 y1 x2 y2 ... x14 y14\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flip the image\n",
    "    f_img = img.copy()\n",
    "    # Flipping the x-axis will simply be to reverse the column order\n",
    "    # NOTE: We use ellipsis to have arbitrary numbers of first dimensions\n",
    "    # https://stackoverflow.com/questions/772124/what-does-the-python-ellipsis-object-do\n",
    "    f_img = f_img[..., ::-1, :]\n",
    "    \n",
    "    # Flip the points along the x-axis by negating the x coordinate\n",
    "    f_points = points.copy()\n",
    "    # NOTE: We use ellipsis to have arbitrary numbers of first dimensions\n",
    "    f_points[..., ::2] = -f_points[..., ::2] \n",
    "    \n",
    "    return f_img, f_points\n",
    "\n",
    "f_img, f_points = flip_img(imgs[1], points[1])\n",
    "visualize_points(f_img, f_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to augment our training sample. Apply flip to every image in training sample. As a result you should obtain two arrays: `aug_imgs_train` and `aug_points_train` which contain original images and points along with flipped ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_imgs_train, aug_points_train = flip_img(imgs_train, points_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_points(aug_imgs_train[2], aug_points_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_points(aug_imgs_train[3], aug_points_train[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecture and training\n",
    "\n",
    "Now let's define neural network regressor. It will have 28 outputs, 2 numbers per point. The precise architecture is up to you. We recommend to add 2-3 (`Conv2D` + `MaxPooling2D`) pairs, then `Flatten` and 2-3 `Dense` layers. Don't forget about ReLU activations. We also recommend to add `Dropout` to every `Dense` layer (with p from 0.2 to 0.5) to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: We only have 6000 images to play with here. As a rule of thumb, more complex networks requires more data in order to learn the complex depenencies. As a consequence we will use a relatively shallow network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, dropout=0.3, outputs=28):\n",
    "    \"\"\"\n",
    "    Returns a uncompiled model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=20,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(filters=20,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=3,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=50,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(filters=50,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=3,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=120,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=3,\n",
    "                           strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(800, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    # NOTE: We use identity on the last layer as we are dealing with a regression problem\n",
    "    model.add(Dense(outputs, activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, loss='mse', optimizer='adadelta'):\n",
    "    \"\"\"\n",
    "    Compiles the model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    loss : str or function\n",
    "        The loss function\n",
    "    optimizer : str or keras.optimizer\n",
    "        The optimizer\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model.compile(loss=loss,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(imgs_train.shape[1:])\n",
    "model = compile_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train! Since we are training a regressor, make sure that you use mean squared error (mse) as loss. Feel free to experiment with optimization method (SGD, Adam, etc.) and its' parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "stopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', baseline=None)\n",
    "checkpointer = ModelCheckpoint(filepath='model.h5',\n",
    "                               verbose=1, \n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training data and validation data\n",
    "x_train = np.concatenate((imgs_train, aug_imgs_train), axis=0)/255\n",
    "x_val = imgs_val/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.concatenate((points_train, aug_points_train), axis=0)\n",
    "y_val = points_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          validation_data=(x_val, y_val),\n",
    "          batch_size=4096,\n",
    "          epochs=100,\n",
    "          shuffle=True,\n",
    "          callbacks=[checkpointer, stopper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualize neural network results on several images from validation sample. Make sure that your network outputs different points for images (i.e. it doesn't output some constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: It would be most appropriate to visualize on a test set as we can overfit to the validation set by trying out different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 10 random images from the validation set\n",
    "indices = random.sample(range(x_val.shape[0]), 10)\n",
    "\n",
    "for i in indices:\n",
    "    predictions = model.predict(x_val)\n",
    "    print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
