{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection\n",
    "\n",
    "Hello! In this task you will create your own deep face detector.\n",
    "\n",
    "First of all, we need import some useful stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have modern Nvidia [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit)? There is your video-card model in [list](https://developer.nvidia.com/cuda-gpus) and CUDA capability >= 3.0?\n",
    "\n",
    "- Yes. You can use it for fast deep learning! In this work we recommend you use tensorflow backend with GPU. Read [installation notes](https://www.tensorflow.org/install/) with attention to gpu section, install all requirements and then install GPU version `tensorflow-gpu`.\n",
    "- No. CPU is enough for this task, but you have to use only simple model. Read [installation notes](https://www.tensorflow.org/install/) and install CPU version `tensorflow`.\n",
    "\n",
    "Of course, also you should install `keras`, `matplotlib`, `numpy` and `scikit-image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_data import load_dataset\n",
    "from get_data import unpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we use processed [FDDB dataset](http://vis-www.cs.umass.edu/fddb/). Processing defined in file [./prepare_data.ipynb](prepare_data.ipynb) and consists of:\n",
    "\n",
    "1. Extract bboxes from dataset. In base dataset face defined by [ellipsis](http://vis-www.cs.umass.edu/fddb/samples/) that not very useful for basic neural network learning.\n",
    "2. Remove images with big and small faces on one shoot.\n",
    "3. Re-size images to bounding boxes (bboxes) have same size 32 +/- pixels.\n",
    "\n",
    "Each image in train, validation and test datasets have shape (176, 176, 3), but part of this image is black background. Interesting image aligned at top left corner.\n",
    "\n",
    "Bounding boxes define face in image and consist of 5 integer numbers: [image_index, min_row, min_col, max_row, max_col]. Bounding box width and height are 32 +/- 8 pixels wide.\n",
    "\n",
    "`train_bboxes` and `val_bboxes` is a list of bboxes.\n",
    "\n",
    "`train_shapes` and `val_shapes` is a list of interesting image shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run will download 30 MB data from github\n",
    "\n",
    "train_images, train_bboxes, train_shapes = load_dataset(\"data\", \"train\")\n",
    "val_images, val_bboxes, val_shapes = load_dataset(\"data\", \"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data (1 point)\n",
    "\n",
    "For learning we should extract positive and negative samples from image.\n",
    "Positive and negative samples counts should be similar.\n",
    "Every samples should have same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from graph import visualize_bboxes\n",
    "visualize_bboxes(images=train_images,\n",
    "                 true_bboxes=train_bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every image can represent multiple faces, so we should extract all faces from every images and crop them to `SAMPLE_SHAPE`. This set of extracted images are named `positive`.\n",
    "\n",
    "Then we chould extract `negative` set. This images should have `SAMPLE_SHAPE` size. Pseudocode for extracting:\n",
    "\n",
    "    negative_collection := []\n",
    "\n",
    "    for i in range(negative_bbox_count):\n",
    "        Select random image.\n",
    "        image_shape := image_shapes[image_index]\n",
    "        image_true_bboxes := true_bboxes[true_bboxes[:, 0] == image_index, 1:]\n",
    "        \n",
    "        for j in TRY_COUNT: # TRY_COUNT is a magic constant, for example, 100\n",
    "            Generate new_bbox within image_shape.\n",
    "            \n",
    "            if new_bbox is negative bbox for image_true_bboxes:\n",
    "                Extract from image, new_bbox and resize to SAMPLE_SIZE negative_sample.\n",
    "                Add negative sample to negative_collection.\n",
    "                Break # for j in TRY_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SHAPE = (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scores import iou_score  # https://en.wikipedia.org/wiki/Jaccard_index\n",
    "\n",
    "def is_negative_bbox(new_bbox, image_bboxes, eps=1e-1):\n",
    "    \"\"\"\n",
    "    Check if new bbox not in true bbox list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    new_bbox : np.array, shape (4,)\n",
    "        The bbox to check on the form\n",
    "        >>> [min_row, min_col, max_row, max_col]\n",
    "    image_bboxes : np.array, shape (n_faces, 4)\n",
    "        Array of the bboxes belonging to the image on the form\n",
    "        >>> [[min_row_1, min_col_1, max_row_1, max_col_1], [min_row_2, min_col_2, max_row_2, max_col_2]...]\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        Wheter or not the new_bbox is a negative (i.e. not overlapping with any face)\n",
    "    \"\"\"\n",
    "    \n",
    "    for bbox in image_bboxes:\n",
    "        if iou_score(new_bbox, bbox) >= eps:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRY_COUNT = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_negative_bbox(image, image_shape, image_bboxes):\n",
    "    \"\"\"\n",
    "    Generate negative bbox for image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.array, shape (cols, rows)\n",
    "        The image under consideration\n",
    "    image_shape : np.array, shape (2,)\n",
    "        Part of the image which contains the non-padded image given as rows (from top), columns (from left)\n",
    "    image_bboxes : np.array, shape (n_faces, 4)\n",
    "        Array of the bboxes belonging to the image on the form\n",
    "        >>> [[min_row_1, min_col_1, max_row_1, max_col_1], [min_row_2, min_col_2, max_row_2, max_col_2]...]\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    success : bool\n",
    "        Wheter or not a negative was generated for the image\n",
    "    negative : np.array, shape (*SAMPLE_SHAPE)\n",
    "        An image not containing enough overlap to be considered as a face\n",
    "    \"\"\"\n",
    "    \n",
    "    # We choose the negative to be a 32x32 square, just like the faces\n",
    "    box_size = SAMPLE_SHAPE[0]\n",
    "    \n",
    "    # Generate list to randomly select width and height from\n",
    "    possible_heights = range(image_shape[0] - box_size)\n",
    "    possible_widths = range(image_shape[1] - box_size)\n",
    "    \n",
    "    found = False\n",
    "    success = True\n",
    "    count = 0\n",
    "    \n",
    "    while not found:\n",
    "        # Generate new box\n",
    "        new_min_row = random.choice(possible_heights)\n",
    "        new_max_row = new_min_row + box_size\n",
    "        new_min_col = random.choice(possible_widths)\n",
    "        new_max_col = new_min_col + box_size\n",
    "        \n",
    "        new_bbox = [new_min_row, new_min_col, new_max_row, new_max_col]\n",
    "        \n",
    "        if is_negative_bbox(new_bbox, image_bboxes):\n",
    "            negative = image[new_min_row:new_max_row, new_min_col:new_max_col]/255\n",
    "            success = True\n",
    "            found = True\n",
    "            \n",
    "        count+=1\n",
    "        \n",
    "        if count == TRY_COUNT:\n",
    "            success = False\n",
    "            negative = np.zeros(SAMPLE_SHAPE)\n",
    "            break\n",
    "    \n",
    "    return success, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positive_negative(images, true_bboxes, image_shapes, negative_bbox_count=None):\n",
    "    \"\"\"\n",
    "    Retrieve positive and negative samples from the images.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    images : list, shape (images, )\n",
    "        List of the images as a numpy array\n",
    "    true_bboxes : np.array, shape (faces, 5)\n",
    "        Bounding boxes belonging to the faces given as\n",
    "        >>> [corresponding_image_index, min_row, min_col, max_row, max_col]\n",
    "    image_shapes : array, shape (2,)\n",
    "        Part of the image file which contains image given as rows (from top), columns (from left)\n",
    "    negative_bbox_count : None or int\n",
    "        Number of bboxes to use.\n",
    "        If None, the number of negative boxes will equal the number of positive boxes\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    positive : list, shape (len(true_bboxes))\n",
    "        List of positive bboxes as np.array with shape SAMPLE_SHAPE\n",
    "    negative : list, shape (negative_bbox_count)\n",
    "        List of negative bboxes as np.array with shape SAMPLE_SHAPE\n",
    "    \"\"\"\n",
    "    \n",
    "    positive = []\n",
    "    negative = []\n",
    "    image_count = image_shapes.shape[0]\n",
    "    \n",
    "    if negative_bbox_count is None:\n",
    "        negative_bbox_count = len(true_bboxes)\n",
    "    \n",
    "    # Extract positive faces\n",
    "    p_bar = tqdm_notebook(true_bboxes)\n",
    "    p_bar.set_description(\"Extracting positive faces\")\n",
    "    for image_index, min_row, min_col, max_row, max_col in p_bar:\n",
    "        # NOTE: Adding 1 to include the endpoint\n",
    "        face = images[image_index][min_row:max_row+1, min_col:max_col+1, :]\n",
    "        positive.append(transform.resize(face, SAMPLE_SHAPE[:2], mode='reflect', anti_aliasing=True))\n",
    "    \n",
    "    # Generate negative faces\n",
    "    image_indices = true_bboxes[:, 0]\n",
    "    p_bar = tqdm_notebook(range(negative_bbox_count))\n",
    "    p_bar.set_description(\"Generating negative faces\")\n",
    "    for i in p_bar:\n",
    "        success = False\n",
    "        while not success:\n",
    "            # Take a random image\n",
    "            image_index = random.choice(image_indices)\n",
    "            # Find bounding boxes for this image\n",
    "            box_indices_this_image = np.where(true_bboxes[:, 0] == image_index)[0]\n",
    "            image_bboxes = np.array([true_bboxes[i, 1:] for i in box_indices_this_image])\n",
    "            \n",
    "            success, neg = gen_negative_bbox(images[image_index],\n",
    "                                             image_shapes[image_index],\n",
    "                                             image_bboxes)\n",
    "\n",
    "        negative.append(neg)\n",
    "        \n",
    "    return positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(images, true_bboxes, image_shapes):\n",
    "    \"\"\"\n",
    "    Obtain data samples and target values\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    images : list, shape (images, )\n",
    "        List of the images as a numpy array\n",
    "    true_bboxes : np.array, shape (faces, 5)\n",
    "        Bounding boxes belonging to the faces given as\n",
    "        >>> [corresponding_image_index, min_row, min_col, max_row, max_col]\n",
    "    image_shapes : array, shape (images, 2)\n",
    "        Part of the image file which contains image given as rows (from top), columns (from left)\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    X : np.array, shape (positive + negative, *SAMPLE_SHAPE)\n",
    "        Positive and negative samples.\n",
    "    Y : np.array, shape (positive + negative, 2)\n",
    "        One hot encoded list of zeros and ones.\n",
    "        Y[sample_nr, 1] == 1 iff the sample is positive\n",
    "    \"\"\"\n",
    "    positive, negative = get_positive_negative(images=images, \n",
    "                                               true_bboxes=true_bboxes, \n",
    "                                               image_shapes=image_shapes)\n",
    "    X = positive\n",
    "    Y = [[0, 1]] * len(positive)\n",
    "    \n",
    "    X.extend(negative)\n",
    "    Y.extend([[1, 0]] * len(negative))\n",
    "    \n",
    "    # NOTE: Cast to float\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract samples from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_samples(train_images, train_bboxes, train_shapes)\n",
    "X_val, Y_val = get_samples(val_images, val_bboxes, val_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There we should see faces\n",
    "from graph import visualize_samples\n",
    "visualize_samples(X_train[Y_train[:, 1] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There we shouldn't see faces\n",
    "visualize_samples(X_train[Y_train[:, 1] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier training (3 points)\n",
    "\n",
    "First of all, we should train face classifier that checks if face represented on sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image augmentation\n",
    "\n",
    "Important thing in deep learning is augmentation. Sometimes, if your model are complex and cool, you can increase quality by using good augmentation.\n",
    "\n",
    "Keras provide good [images preprocessing and augmentation](https://keras.io/preprocessing/image/). This preprocessing executes online (on the fly) while learning.\n",
    "\n",
    "Of course, if you want using samplewise and featurewise center and std normalization you should run this transformation on predict stage. But you will use this classifier to fully convolution detector, in this case such transformation quite complicated, and we don't recommend use them in classifier.\n",
    "\n",
    "For heavy augmentation you can use library [imgaug](https://github.com/aleju/imgaug). If you need, you can use this library in offline manner (simple way) and online manner (hard way). However, hard way is not so hard: you only have to write [python generator](https://wiki.python.org/moin/Generators), which returns image batches, and pass it to [fit_generator](https://keras.io/models/model/#fit_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator # Usefull thing. Read the doc.\n",
    "\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             samplewise_center=True,\n",
    "                             samplewise_std_normalization=True)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting classifier\n",
    "\n",
    "For fitting you can use one of Keras optimizer algorithms. [Good overview](http://ruder.io/optimizing-gradient-descent/)\n",
    "\n",
    "To choose best learning rate strategy you should read about EarlyStopping and ReduceLROnPlateau or LearningRateScheduler on [callbacks](https://keras.io/callbacks/) page of keras documentation, it's very useful in deep learning.\n",
    "\n",
    "If you repeat architecture from some paper, you can find information about good optimizer algorithm and learning rate strategy in this paper. For example, every [keras application](https://keras.io/applications/) has link to paper, that describes suitable learning procedure for this specific architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint(model_name, \n",
    "                   output_dir=Path('.').absolute().joinpath('data', 'checkpoints')):\n",
    "    \"\"\"\n",
    "    Returns the model which achieved the best validation score for a given modelname\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str\n",
    "        The name of the model\n",
    "    output_dir : Path or str\n",
    "        Name of the output directory\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    checkpoint_path : str or None\n",
    "        The checkpoint path given as a string\n",
    "    \"\"\"\n",
    "    \n",
    "    checkpoints = list(Path(output_dir).glob(f'{model_name}*'))\n",
    "    checkpoints = [c for c in checkpoints if '.pkl' not in str(c)]\n",
    "    \n",
    "    if len(list(checkpoints)) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Sort after lowest validation\n",
    "    checkpoint = sorted(checkpoints,\n",
    "                        key=lambda checkpoint: str(checkpoint).split('_')[-1].split('.hdf5')[0])\n",
    "        \n",
    "    return str(checkpoint[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph import plot_history\n",
    "import pickle\n",
    "\n",
    "def fit(model, \n",
    "        datagen,\n",
    "        X_train, \n",
    "        Y_train, \n",
    "        X_val, \n",
    "        Y_val,\n",
    "        output_dir=Path('.').absolute().joinpath('data', 'checkpoints'),\n",
    "        epochs=50, \n",
    "        lr=0.002,\n",
    "        verbose=True,\n",
    "        force_run=False,\n",
    "        model_name=None,\n",
    "        early_stop_patience=None,\n",
    "        lr_on_plateau_patience=None,\n",
    "        class_weight=None):\n",
    "    \"\"\"\n",
    "    Fits the model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Sequential\n",
    "        The model to fit\n",
    "    datagen : ImageDataGenerator\n",
    "        The datagenerator for image augmentation\n",
    "    X_train : np.array, shape (train, 32, 32, 3)\n",
    "        The training data\n",
    "    Y_train : np.array, shape (train, 2)\n",
    "        The target values\n",
    "    X_val : np.array, shape (val, 32, 32, 3)\n",
    "        The validation data\n",
    "    Y_val : np.array, shape (val, 2)\n",
    "        The target used for validation\n",
    "    output_dir : Path or str\n",
    "        Name of the output directory\n",
    "    epochs : int\n",
    "        The number of epochs to use for training\n",
    "    lr : float or function\n",
    "        The learning rate\n",
    "        If lr is a function it must accept epochs and output a learningrate\n",
    "    verbose : bool\n",
    "        Whether or not to print summary of the model\n",
    "    force_run : bool\n",
    "        Will fit the model, even if a model checkpoint is found\n",
    "    model_name : str\n",
    "        Name of model (save name of the model)\n",
    "    early_stop_patience : int\n",
    "        Patience for early stop\n",
    "    lr_on_plateau_patience : int\n",
    "        Patience for ReduceLROnPlateau\n",
    "    class_weight : None or dict\n",
    "        Optional dictionary mapping class indices (integers) to a weight (float) value, \n",
    "        used for weighting the loss function (during training only). \n",
    "        This can be useful to tell the model to \"pay more attention\" to samples from an \n",
    "        under-represented class\n",
    "    \"\"\"\n",
    "    \n",
    "    if not output_dir.is_dir():\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "\n",
    "    # Check if the model has already been trained\n",
    "    checkpoint = get_checkpoint(model_name, output_dir)\n",
    "    history_path = Path(output_dir).joinpath(f'{model_name}.pkl')\n",
    "    if model_name is not None and checkpoint is not None and not force_run:\n",
    "        print(f'Found the best weights stored in {checkpoint}')\n",
    "        with history_path.open('rb') as f:\n",
    "            history = pickle.load(f)\n",
    "            plot_history(history)\n",
    "        return\n",
    "        \n",
    "    if callable(lr):\n",
    "        init_lr = 0.002\n",
    "    else:\n",
    "        init_lr = lr\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=init_lr),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    callbacks = []\n",
    "    \n",
    "    if model_name is not None:\n",
    "        callbacks.append(ModelCheckpoint(str(Path(output_dir).joinpath(f'{model_name}')) +\n",
    "                                 '-{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True))\n",
    "        \n",
    "    if early_stop_patience is not None:\n",
    "        callbacks.append(EarlyStopping(monitor='val_loss',\n",
    "                                       patience=early_stop_patience,\n",
    "                                       verbose=1,\n",
    "                                       mode='auto',\n",
    "                                       baseline=None))\n",
    "        \n",
    "    if lr_on_plateau_patience is not None:\n",
    "        callbacks.append(ReduceLROnPlateau(monitor='val_loss',\n",
    "                                           factor=0.1,\n",
    "                                           patience=lr_on_plateau_patience, \n",
    "                                           verbose=1,\n",
    "                                           mode='auto', \n",
    "                                           min_delta=0.001, \n",
    "                                           cooldown=0, \n",
    "                                           min_lr=0))\n",
    "    \n",
    "    if callable(lr):\n",
    "        callbacks.append(LearningRateScheduler(lr, verbose=1))\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "        datagen.flow(X_train,\n",
    "                     Y_train,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     shuffle=True),\n",
    "        validation_data=(datagen.standardize(X_val),\n",
    "                         Y_val),\n",
    "        epochs=epochs, \n",
    "        steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight)\n",
    "\n",
    "    with history_path.open('wb') as f:\n",
    "        pickle.dump(history.history, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    plot_history(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (first point out of three)\n",
    "\n",
    "![lenet architecture](lenet_architecture.png)\n",
    "LeCun, Y., Bottou, L., Bengio, Y. and Haffner, P., 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), pp.2278-2324.\n",
    "\n",
    "Of course, you can use any another architecture, if want. Main thing is classification quality of your model.\n",
    "\n",
    "Acceptable validation accuracy for this task is 0.92."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet(input_shape, dropout=0.3, outputs=2):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled lenet-like model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    dropout : float\n",
    "        Degree of dropout\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=20,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=50,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(outputs, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steroid_lenet(input_shape, dropout=0.3, outputs=2):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled lenet-like model, but with a couple of modifications\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    dropout : float\n",
    "        Degree of dropout\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=20,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=50,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=120,\n",
    "                     kernel_size=5,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(800, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(outputs, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar10(input_shape, dropout=0.3, outputs=2):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled cifar10-like model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    dropout : float\n",
    "        Degree of dropout\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(outputs, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_cifar10(input_shape, dropout=0.2, outputs=2):\n",
    "    \"\"\"\n",
    "    Returns an uncompiled cifar10-like model, with an extra dense layer\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    dropout : float\n",
    "        Degree of dropout\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(outputs, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_cifar10(input_shape, dropout=0.3, outputs=2):\n",
    "    \"\"\"\n",
    "    Returns a large, uncompiled cifar10-like model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : array-like, shape (3,)\n",
    "        Tuple containing height, width and depth\n",
    "    dropout : float\n",
    "        Degree of dropout\n",
    "    outputs : int\n",
    "        Number of outputs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : Sequential\n",
    "        The uncompiled model\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    https://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    \n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=32,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=64,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "    \n",
    "    model.add(Conv2D(filters=128,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=128,\n",
    "                     kernel_size=3,\n",
    "                     strides=1,\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2,\n",
    "                           strides=2))\n",
    "  \n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(outputs, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_model = lenet(SAMPLE_SHAPE)\n",
    "steroid_lenet_model = steroid_lenet(SAMPLE_SHAPE)\n",
    "cifar10_model = cifar10(SAMPLE_SHAPE)\n",
    "improved_cifar10_model = improved_cifar10(SAMPLE_SHAPE)\n",
    "big_cifar10_model = big_cifar10(SAMPLE_SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model (second point out of three)\n",
    "\n",
    "If you doesn't have fast video-card suitable for deep learning, you can first check neural network modifications with small value of parameter `epochs`, for example, 10, and then after selecting best model increase this parameter.\n",
    "Fitting on CPU can be long, we suggest do it at bedtime.\n",
    "\n",
    "Don't forget change model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit(model=lenet_model, \n",
    "    datagen=datagen,\n",
    "    X_train=X_train, \n",
    "    X_val=X_val, \n",
    "    Y_train=Y_train,\n",
    "    Y_val=Y_val,\n",
    "    epochs=50, \n",
    "    lr=0.002,\n",
    "    model_name='lenet',\n",
    "    early_stop_patience=10,\n",
    "    lr_on_plateau_patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit(model=steroid_lenet_model, \n",
    "    datagen=datagen,\n",
    "    X_train=X_train, \n",
    "    X_val=X_val, \n",
    "    Y_train=Y_train,\n",
    "    Y_val=Y_val,\n",
    "    epochs=50, \n",
    "    lr=0.002,\n",
    "    model_name='steroid_lenet',\n",
    "    early_stop_patience=10,\n",
    "    lr_on_plateau_patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit(model=cifar10_model, \n",
    "    datagen=datagen,\n",
    "    X_train=X_train, \n",
    "    X_val=X_val, \n",
    "    Y_train=Y_train,\n",
    "    Y_val=Y_val,\n",
    "    epochs=50, \n",
    "    lr=0.002,\n",
    "    model_name='cifar10',\n",
    "    early_stop_patience=10,\n",
    "    lr_on_plateau_patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit(model=improved_cifar10_model, \n",
    "    datagen=datagen,\n",
    "    X_train=X_train, \n",
    "    X_val=X_val, \n",
    "    Y_train=Y_train,\n",
    "    Y_val=Y_val,\n",
    "    epochs=50, \n",
    "    lr=0.002,\n",
    "    model_name='improved_cifar10',\n",
    "    early_stop_patience=10,\n",
    "    lr_on_plateau_patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit(model=big_cifar10_model, \n",
    "    datagen=datagen,\n",
    "    X_train=X_train, \n",
    "    X_val=X_val, \n",
    "    Y_train=Y_train,\n",
    "    Y_val=Y_val,\n",
    "    epochs=50, \n",
    "    lr=0.002,\n",
    "    model_name='big_cifar10',\n",
    "    early_stop_patience=10,\n",
    "    lr_on_plateau_patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (third point out of three)\n",
    "\n",
    "After learning model weights saves in folder `data/checkpoints/`.\n",
    "Use `model.load_weights(fname)` to load best weights.\n",
    "\n",
    "If you use Windows and Model Checkpoint doesn't work on your configuration, you should implement [your own Callback](https://keras.io/callbacks/#create-a-callback) to save best weights in memory and then load it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: get_checkpoint is defined above for use in fit\n",
    "model = cifar10_model\n",
    "model.load_weights(get_checkpoint('cifar10'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection\n",
    "\n",
    "If you have prepared classification architecture with high validation score, you can use this architecture for detection.\n",
    "\n",
    "Convert classification architecture to fully convolution neural network (FCNN), that returns heatmap of activation.\n",
    "\n",
    "### Detector model or sliding window (1 point)\n",
    "\n",
    "Now you should replace fully-connected layers with $1 \\times 1$ convolution layers.\n",
    "\n",
    "Every fully connected layer perform operation $f(Wx + b)$, where $f(\\cdot)$ is nonlinear activation function, $x$ is layer input, $W$ and $b$ is layer weights. This operation can be emulated with $1 \\times 1$ convolution with activation function $f(\\cdot)$, that perform exactly same operation $f(Wx + b)$.\n",
    "\n",
    "If there is `Flatten` layer with $n \\times k$ input size before fully connected layers, convolution should have same $n \\times k$ input size.\n",
    "Multiple fully connected layers can be replaced with convolution layers sequence.\n",
    "\n",
    "After replace all fully connected layers with convolution layers, we get fully convolution network. If input shape is equal to input size of previous network, output will have size $1 \\times 1$. But if we increase input shape, output shape automatically will be increased. For example, if convolution step of previous network strides 4 pixels, increase input size with 100 pixels along all axis makes increase outputsize with 25 values along all axis. We got activation map of classifier without necessary extract samples from image and multiple calculate low-level features.\n",
    "\n",
    "In total:\n",
    "1. $1 \\times 1$ convolution layer is equivalent of fully connected layer.\n",
    "2. $1 \\times 1$ convolution layers can be used to get activation map of classification network in \"sliding window\" manner.\n",
    "\n",
    "We propose replace last fully connected layer with softmax actiovation to convolution layer with linear activation.It will be usefull to find good treshold. Of course, you can use softmax activation.\n",
    "\n",
    "#### Example of replace cnn head:\n",
    "\n",
    "##### Head before convert\n",
    "\n",
    "![before replace image](before_convert.png)\n",
    "\n",
    "##### Head after convert\n",
    "\n",
    "![before replace image](after_convert.png)\n",
    "\n",
    "On this images displayed only head. `InputLayer` should be replaced with convolution part exit.\n",
    "Before convert network head takes fifty $8 \\times 8$ feature maps and returns two values: probability of negative and positive classes. This output can be considered as activation map with size $1 \\times 1$.\n",
    "\n",
    "If input have size $8 \\times 8$, output after convert would have $1 \\times 1$ size, but input size is $44 \\times 44$.\n",
    "After convert network head returns one $37 \\times 37$ activation map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (176, 176, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fcnn_model(model, input_shape, last_activation='linear', verbose=True):\n",
    "    \"\"\"\n",
    "    Generate a fully convolutional neural network from a network with part conv layers and part fc layers\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    See\n",
    "    http://cs231n.github.io/convolutional-networks/#convert\n",
    "    for better explaination\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : tuple, shape (3,)\n",
    "        The input shape for the model\n",
    "    model : Sequantial\n",
    "        The model to transform\n",
    "    last_activation : str\n",
    "        The activation for the last layer\n",
    "    verbose : bool\n",
    "        Whether or not to print the summary\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    new_model : Sequential\n",
    "        The uncompiled transformed model\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    An alternative way of doing this, including setting weights\n",
    "    https://stackoverflow.com/questions/41161021/how-to-convert-a-dense-layer-to-an-equivalent-convolutional-layer-in-keras\n",
    "    \"\"\"\n",
    "    \n",
    "    prev_layer_flatten = False\n",
    "    \n",
    "    new_model = Sequential()\n",
    "\n",
    "    # Special case for first layer\n",
    "    layer_conf = model.layers[0].get_config()\n",
    "    new_model.add(Conv2D(filters=layer_conf['filters'],\n",
    "                         kernel_size=layer_conf['kernel_size'],\n",
    "                         strides=layer_conf['strides'],\n",
    "                         padding=layer_conf['padding'],\n",
    "                         activation=layer_conf['activation'],\n",
    "                         input_shape=input_shape))\n",
    "    \n",
    "    # Loop through all intermediate layers\n",
    "    for layer in model.layers[1:-1]:\n",
    "        layer_name = str(layer).split('.')[-1].split(' ')[0]\n",
    "        layer_conf = layer.get_config()\n",
    "        \n",
    "        if layer_name == 'Dropout':\n",
    "            new_model.add(Dropout(layer.get_config()['rate']))\n",
    "        elif layer_name == 'BatchNormalization':\n",
    "            new_model.add(BatchNormalization())\n",
    "        elif layer_name == 'Conv2D':\n",
    "            new_model.add(Conv2D(filters=layer_conf['filters'],\n",
    "                                 kernel_size=layer_conf['kernel_size'],\n",
    "                                 strides=layer_conf['strides'],\n",
    "                                 padding=layer_conf['padding'],\n",
    "                                 activation=layer_conf['activation']))\n",
    "        elif layer_name == 'MaxPooling2D':\n",
    "            new_model.add(MaxPooling2D(pool_size=layer_conf['pool_size'],\n",
    "                                       strides=layer_conf['strides']))\n",
    "        elif layer_name == 'Flatten':\n",
    "            prev_layer_flatten = True\n",
    "            flatten_kernel_size = layer.input_shape[1]\n",
    "        elif layer_name == 'Dense':\n",
    "            if prev_layer_flatten:\n",
    "                # Flattening is replaced with a filter-size equaling the last filter size before the flattening \n",
    "                # layer, keeping the number of filters constant\n",
    "                # NOTE: Padding must be set to valid in order for the dimensions to match\n",
    "                new_model.add(Conv2D(filters=layer_conf['units'],\n",
    "                                     kernel_size=flatten_kernel_size,\n",
    "                                     strides=1,\n",
    "                                     padding='valid',\n",
    "                                     activation=layer_conf['activation']))\n",
    "                prev_layer_flatten = False\n",
    "            else:\n",
    "                # Dense layer with x nodes will be replaced with x 1x1 filters\n",
    "                new_model.add(Conv2D(filters=layer_conf['units'],\n",
    "                                     kernel_size=1,\n",
    "                                     strides=1,\n",
    "                                     padding='valid',\n",
    "                                     activation=layer_conf['activation']))                \n",
    "        else:\n",
    "            raise NotImplementedError(f'{layer_name} not implemented')\n",
    "\n",
    "    # Special case for last layer as we may want linear activation\n",
    "    new_model.add(Conv2D(filters=model.layers[-1].get_config()['units'],\n",
    "                         kernel_size=1,\n",
    "                         strides=1,\n",
    "                         padding='valid',\n",
    "                         activation=last_activation))            \n",
    "            \n",
    "    if verbose:\n",
    "        new_model.summary()\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fcnn_model = generate_fcnn_model(model, IMAGE_SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1 point)\n",
    "\n",
    "Then you should write function that copy weights from classification model to fully convolution model.\n",
    "Convolution weights may be copied without modification, fully-connected layer weights should be reshaped before copy.\n",
    "\n",
    "Pay attention to last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_weights(base_model, fcnn_model):\n",
    "    \"\"\"\n",
    "    Copy weigths from a model with a mixture of convolution and dense layers \n",
    "    to the equivalent consisting of only covolutional layers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    base_model : Sequential\n",
    "        The models to copy the weights from\n",
    "    fcnn_model : Sequential\n",
    "        The model to copy the weights to\n",
    "    \"\"\"\n",
    "    \n",
    "    base_config = base_model.get_config()\n",
    "    fcnn_config = fcnn_model.get_config()\n",
    "    \n",
    "    # We operate with two counters: Whenever there are Flattening layers, the fcnn counter will lag by one\n",
    "    j = 0\n",
    "    for i in range(len(base_config)):\n",
    "        base_name = base_config[i]['class_name']\n",
    "        fcnn_name = fcnn_config[j]['class_name']\n",
    "        \n",
    "        if base_name == fcnn_name:\n",
    "            fcnn_model.layers[j].set_weights(base_model.layers[i].get_weights())\n",
    "        elif base_name == 'Flatten':\n",
    "            # NOTE: j will not be updated\n",
    "            continue\n",
    "        elif base_name == 'Dense' and fcnn_name == 'Conv2D':\n",
    "            fcnn_w_dims = fcnn_model.layers[j].get_weights()[0].shape\n",
    "            fcnn_b_dims = fcnn_model.layers[j].get_weights()[1].shape\n",
    "            \n",
    "            new_w_b = [base_model.layers[i].get_weights()[0].reshape(fcnn_w_dims),\n",
    "                       base_model.layers[i].get_weights()[1].reshape(fcnn_b_dims)]\n",
    "            \n",
    "            fcnn_model.layers[j].set_weights(new_w_b)\n",
    "        else:\n",
    "            raise RuntimeError(f'Mismatch of layers base {i}: {base_name}, fcnn {j}: {fcnn_name}')\n",
    "        \n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_weights(base_model=model, fcnn_model=fcnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph import visualize_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = fcnn_model.predict(datagen.standardize(np.array(val_images)[:6, ...].astype(float)))\n",
    "visualize_heatmap(val_images, predictions[:, :, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detector (1 point)\n",
    "\n",
    "First detector part is getting bboxes and decision function.\n",
    "Greater decision function indicates better detector confidence.\n",
    "\n",
    "This function should return pred_bboxes and decision_function:\n",
    "\n",
    "- `pred bboxes` is list of 5 int tuples like `true bboxes`: `[image_index, min_row, min_col, max_row, max_col]`.\n",
    "- `decision function` is confidence of detector for every pred bbox: list of float values, `len(decision function) == len(pred bboxes)` \n",
    " \n",
    "We propose resize image to `IMAGE_SHAPE` size, find faces on resized image with `SAMPLE_SHAPE` size and then resize them back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fccn_predictions(fcnn_model, images):\n",
    "    \"\"\"\n",
    "    Returns the predicted bounding boxes and decision functions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fcnn : Sequential\n",
    "        The full CNN model to use for prediction\n",
    "    images : list, shape (n_images, )\n",
    "        List of images, where each element is a np.array with shape (width, height, 3)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    fcnn_predictions : np.array, shape (n_images, n_cols, n_rows, 2)\n",
    "        Predictions from the full-cnn model\n",
    "    \"\"\"\n",
    "    \n",
    "    # NOTE: Ternary operator as the result of a list comprehension\n",
    "    resized_images = np.array([transform.resize(image,\n",
    "                                                IMAGE_SHAPE,\n",
    "                                                mode=\"reflect\",\n",
    "                                                anti_alias=True)\n",
    "                               if image.shape != IMAGE_SHAPE else\n",
    "                               image for image in images])\n",
    "\n",
    "    # Predict\n",
    "    fcnn_predictions = fcnn_model.predict(datagen.standardize(resized_images.astype(float)))\n",
    "    \n",
    "    return fcnn_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "def get_bboxes_and_decision_function(fcnn_predictions, image_shapes):\n",
    "    \"\"\"\n",
    "    Returns the predicted bounding boxes and decision functions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fcnn_predictions : np.array, shape (n_images, n_cols, n_rows, 2)\n",
    "        Predictions from the full-cnn model\n",
    "    image_shapes : array, shape (images, 2)\n",
    "        Part of the image file which contains image given as rows (from top), columns (from left)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pred_bboxes : list, shape (n_pred_face, )\n",
    "        List of all predictions.\n",
    "        Each element is on the form [image_index, min_row, min_col, max_row, max_col]\n",
    "    decision_function : list, shape (n_pred_face, )\n",
    "        List of confidence for every predicted bounding box\n",
    "    \"\"\"    \n",
    "    pred_bboxes, decision_function = [], []\n",
    "    \n",
    "    pred_shape = fcnn_predictions[0, :, :, 0].shape\n",
    "    \n",
    "    # Pick the prediction for true face (i.e. index 1)\n",
    "    for i, prediction in enumerate(fcnn_predictions[:, :, :, 1]):\n",
    "        coordinates = peak_local_max(prediction,\n",
    "                                     min_distance=1,\n",
    "                                     indices=True,\n",
    "                                     num_peaks=8)\n",
    "        \n",
    "        for h, w in coordinates:\n",
    "            image_index = i\n",
    "            \n",
    "            confidence = prediction[h, w]\n",
    "            \n",
    "            # Rescale the height and the width\n",
    "            h = np.round(h*IMAGE_SHAPE[1]/pred_shape[1]).astype(int)\n",
    "            w = np.round(w*IMAGE_SHAPE[0]/pred_shape[0]).astype(int)\n",
    "            \n",
    "            # NOTE: peak_local_max does not return the center of the max,\n",
    "            #       but rather the upper left edge of the blob\n",
    "            min_row = (h).clip(0, IMAGE_SHAPE[1])\n",
    "            max_row = (h + SAMPLE_SHAPE[1]).clip(0, IMAGE_SHAPE[1])\n",
    "            min_col = (w).clip(0, IMAGE_SHAPE[0])\n",
    "            max_col = (w + SAMPLE_SHAPE[0]).clip(0, IMAGE_SHAPE[0])\n",
    "            \n",
    "            \n",
    "            pred_bboxes.append((image_index, min_row, min_col, max_row, max_col))\n",
    "            decision_function.append(confidence)\n",
    "            \n",
    "    return pred_bboxes, decision_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detector visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnn_predictions_val = get_fccn_predictions(fcnn_model=fcnn_model, images=val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bboxes_val, decision_function_val = get_bboxes_and_decision_function(fcnn_predictions=fcnn_predictions_val,\n",
    "                                                                          image_shapes=val_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_bboxes(images=val_images,\n",
    "                 pred_bboxes=pred_bboxes_val,\n",
    "                 true_bboxes=val_bboxes,\n",
    "                 decision_function=decision_function_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detector score (1 point)\n",
    "\n",
    "Write [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) graph.\n",
    "\n",
    "You can use function `best_match` to extract matching between prediction and ground truth, false positive and false negative samples. Pseudo-code for calculation precision and recall graph:\n",
    "    \n",
    "    # Initialization for first step threshold := -inf\n",
    "    tn := 0 # We haven't any positive sample\n",
    "    fn := |false_negative| # But some faces wasn't been detected\n",
    "    tp := |true_bboxes| # All true bboxes have been caught\n",
    "    fp := |false_positive| # But also some false positive samples have been caught\n",
    "    \n",
    "    Sort decision_function and pred_bboxes with order defined by decision_function\n",
    "    y_true := List of answers for \"Is the bbox have matching in y_true?\" for every bbox in pred_bboxes\n",
    "    \n",
    "    for y_on_this_step in y_true:\n",
    "        # Now we increase threshold, so some predicted bboxes makes positive.\n",
    "        # If y_t is True then the bbox is true positive else bbox is false positive\n",
    "        # So we should\n",
    "        Update tp, tn, fp, fn with attention to y_on_this_step\n",
    "        \n",
    "        Add precision and recall point calculated by formula through tp, tn, fp, fn on this step\n",
    "        Threshold for this point is decision function on this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_curve(pred_bboxes, true_bboxes, decision_functions, overlap_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Generates the precision and recall for a corresponding decision function\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pred_bboxes : list, shape (n_pred_face, 5)\n",
    "        List of all predictions.\n",
    "        Each element is on the form [image_index, min_row, min_col, max_row, max_col]\n",
    "    true_bboxes : np.array, shape (n_faces, 5)\n",
    "        Bounding boxes belonging to the faces given as\n",
    "        >>> [corresponding_image_index, min_row, min_col, max_row, max_col]\n",
    "    decision_functions : list, shape (n_pred_face, )\n",
    "        List of confidence for every predicted bounding box\n",
    "    overlap_treshold : float\n",
    "        IoU-threshold for true positive\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    precisions : list, shape (n_pred_face, )\n",
    "        List of the precisions corresponding to a treshold of the decision boundray\n",
    "    recalls : list, shape (n_pred_face, )\n",
    "        List of the recalls corresponding to a treshold of the decision boundray\n",
    "    thresholds : list, shape (n_pred_face, )\n",
    "        List of the tresholds of the decision boundray\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    thresholds = []\n",
    "\n",
    "    # Sort the decision function in descending order\n",
    "    # NOTE: The indices of the pred_bboxes are sorted together with the pred_bboxes\n",
    "    #       In this way, we can find the best matching predicted box to an image \n",
    "    #       (the one with the highest decision_function)\n",
    "    #       On hit, we mark the true_bbox as found so that only one prediction can match a true_bbox for all\n",
    "    #       decision_functions\n",
    "    decision_functions, pred_bboxes = zip(*sorted(zip(decision_functions, pred_bboxes), reverse=True))\n",
    "    \n",
    "    true_bbox_image_indices = np.array([true_bbox[0] for true_bbox in true_bboxes], dtype=int)\n",
    "\n",
    "    num_true_bboxes = true_bboxes.shape[0]\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    # Loop over all predictions\n",
    "    for prediction_bbox, decision_function in zip(pred_bboxes, decision_functions):\n",
    "        cur_image_index = prediction_bbox[0]\n",
    "        \n",
    "        # Find true_bboxes which matches cur_image_index\n",
    "        matching_pred_indices = np.where(true_bbox_image_indices == cur_image_index)[0]\n",
    "        true_bbox_candidates = true_bboxes[matching_pred_indices]\n",
    "        \n",
    "        max_overlap = 0\n",
    "        index_with_max_overlap = None\n",
    "        for true_box_index, true_bbox_candidate in enumerate(true_bbox_candidates):\n",
    "            match_score = iou_score(true_bbox_candidate[1:], prediction_bbox[1:])\n",
    "            if match_score > max_overlap:\n",
    "                max_overlap = match_score\n",
    "                index_with_max_overlap = true_box_index\n",
    "\n",
    "        if max_overlap > overlap_threshold:\n",
    "            tp += 1\n",
    "            # Match the true bounding box as found (so that it will not be found in matching_pred_indices)\n",
    "            # I.e. there can be no other candidates with lower decision_function that can match this true_bbox\n",
    "            true_bbox_image_indices[matching_pred_indices[index_with_max_overlap]] = -1\n",
    "        else:\n",
    "            fp += 1\n",
    "            \n",
    "        # n_true_bboxes = tp + fn \n",
    "        # fn = n_true_bboxes - tp \n",
    "        fn = len(true_bbox_image_indices) - tp\n",
    "        \n",
    "        # NOTE: Adding machine epsilon to avoid division by 0\n",
    "        pre = tp / (tp+fp+np.finfo(float).eps)\n",
    "        rec = tp / (tp+fn+np.finfo(float).eps)\n",
    "        \n",
    "        precisions.append(pre)\n",
    "        recalls.append(rec)\n",
    "        thresholds.append(decision_function)\n",
    "    \n",
    "    return precisions, recalls, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph import plot_precision_recall\n",
    "precision_val, recall_val, thresholds_val = precision_recall_curve(pred_bboxes=pred_bboxes_val, \n",
    "                                                                   true_bboxes=val_bboxes,\n",
    "                                                                   decision_functions=decision_function_val)\n",
    "plot_precision_recall(precision=precision_val, recall=recall_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold (1 point)\n",
    "\n",
    "Next step in detector creating is select threshold for decision_function.\n",
    "Every possible threshold presents point on recall-precision graph.\n",
    "\n",
    "Select threshold for `recall=0.85`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold(thresholds, recall, match_recall=0.85):\n",
    "    \"\"\"\n",
    "    Obtain a threshold value where the recall is the closest, but at least equal to match_recall\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    thresholds must be sorted in descending order\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    thresholds : list, shape (n_pred_face, )\n",
    "        List of the tresholds of the decision boundray\n",
    "    recalls : list, shape (n_pred_face, )\n",
    "        List of the recalls corresponding to a treshold of the decision boundray \n",
    "    match_recall : float\n",
    "        Value which should at least be matched\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    threshold : float\n",
    "        The threshold which matches the criteria stated above\n",
    "    \"\"\"\n",
    "    \n",
    "    possible_thresholds = np.where(np.array(recall) >= match_recall)[0]\n",
    "    threshold = thresholds[possible_thresholds[0]]\n",
    "    \n",
    "    return threshold\n",
    "\n",
    "THRESHOLD = get_threshold(thresholds_val, recall_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(pred_bboxes, decision_function, threshold, return_decision=True):\n",
    "    \"\"\"\n",
    "    Get bboxes with decision_function not lesser than the threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred_bboxes : list, shape (n_pred_face, )\n",
    "        List of all predictions.\n",
    "        Each element is on the form [image_index, min_row, min_col, max_row, max_col]\n",
    "    decision_function : list, shape (n_pred_face, )\n",
    "        List of confidence for every predicted bounding box\n",
    "    threshold : float\n",
    "        The threshold on the decision_function for valid bboxes\n",
    "    return_decision : bool\n",
    "        If True, the valid desicions are returned\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    threshold : float\n",
    "        The threshold which matches the criteria stated above\n",
    "    \"\"\"\n",
    "\n",
    "    valid_ind = np.where(np.array(decision_function) >= threshold)[0]\n",
    "    result_decision = np.array(decision_function)[valid_ind]\n",
    "    result = np.array(pred_bboxes)[valid_ind]\n",
    "    \n",
    "    if return_decision:\n",
    "        return result, result_decision\n",
    "    else:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bboxes, decision_function = detect(pred_bboxes=pred_bboxes_val,\n",
    "                                        decision_function=decision_function_val,\n",
    "                                        threshold=THRESHOLD, \n",
    "                                        return_decision=True)\n",
    "\n",
    "visualize_bboxes(images=val_images,\n",
    "                 pred_bboxes=pred_bboxes,\n",
    "                 true_bboxes=val_bboxes,\n",
    "                 decision_function=decision_function\n",
    "                )\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(pred_bboxes=pred_bboxes, \n",
    "                                                       true_bboxes=val_bboxes, \n",
    "                                                       decision_functions=decision_function)\n",
    "plot_precision_recall(precision=precision, \n",
    "                      recall=recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset (1 point)\n",
    "\n",
    "Last detector preparation step is testing.\n",
    "\n",
    "Attention: to avoid over-fitting, after testing algorithm you should run [./prepare_data.ipynb](prepare_data.ipynb), and start all fitting from beginning.\n",
    "\n",
    "Detection score (in graph header) should be 0.85 or greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_bboxes, test_shapes = load_dataset(\"data\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We test get_bboxes_and_decision_function becouse we want pay attention to all recall values\n",
    "fcnn_predictions_test = get_fccn_predictions(fcnn_model=fcnn_model, images=test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bboxes_test, decision_function_test = get_bboxes_and_decision_function(fcnn_predictions=fcnn_predictions_test,\n",
    "                                                                            image_shapes=test_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_bboxes(images=test_images,\n",
    "                 pred_bboxes=pred_bboxes_test,\n",
    "                 true_bboxes=test_bboxes,\n",
    "                 decision_function=decision_function_test)\n",
    "\n",
    "precision_test, recall_test, _ = precision_recall_curve(pred_bboxes=pred_bboxes_test, \n",
    "                                                        true_bboxes=test_bboxes, \n",
    "                                                        decision_functions=decision_function_test)\n",
    "plot_precision_recall(precision=precision_test, \n",
    "                      recall=recall_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional tasks\n",
    "\n",
    "### Real image dataset\n",
    "\n",
    "Test your algorithm on original (not scaled) data.\n",
    "Visualize bboxes and plot precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run will download 523 MB data from github\n",
    "\n",
    "original_images, original_bboxes, original_shapes = load_dataset(\"data\", \"original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard negative mining\n",
    "\n",
    "Upgrade the score with [hard negative mining](https://www.reddit.com/r/computervision/comments/2ggc5l/what_is_hard_negative_mining_and_how_is_it/).\n",
    "\n",
    "A hard negative is when you take that falsely detected patch, and explicitly create a negative example out of that patch, and add that negative to your training set. When you retrain your classifier, it should perform better with this extra knowledge, and not make as many false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write this function\n",
    "def hard_negative(train_images, image_shapes, train_bboxes, X_val, Y_val, base_model, fcnn_model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_negative(train_images=train_images, image_shapes=train_shapes, train_bboxes=train_bboxes, X_val=X_val, Y_val=Y_val, base_model=model, fcnn_model=fcnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"data/checkpoints/...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_weights(base_model=model, fcnn_model=fcnn_model)\n",
    "\n",
    "pred_bboxes, decision_function = get_bboxes_and_decision_function(fcnn_model=fcnn_model, images=val_images, image_shapes=val_shapes)\n",
    "\n",
    "visualize_bboxes(images=val_images,\n",
    "                 pred_bboxes=pred_bboxes,\n",
    "                 true_bboxes=val_bboxes,\n",
    "                 decision_function=decision_function\n",
    "                )\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(pred_bboxes=pred_bboxes, true_bboxes=val_bboxes, decision_function=decision_function)\n",
    "plot_precision_recall(precision=precision, recall=recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Multi scale detector\n",
    "\n",
    "Write and test detector with [pyramid representation][pyramid].\n",
    "[pyramid]: https://en.wikipedia.org/wiki/Pyramid_(image_processing)\n",
    "\n",
    "1. Resize images to predefined scales.\n",
    "2. Run detector with different scales.\n",
    "3. Apply non-maximum supression to detection on different scales.\n",
    "\n",
    "References:\n",
    "1. [E. H. Adelson,C. H. Anderson, J. R. Bergen, P. J. Burt, J. M. Ogden: Pyramid methods in image processing](http://persci.mit.edu/pub_pdfs/RCA84.pdf)\n",
    "2. [PETER J. BURT, EDWARD H. ADELSON: The Laplacian Pyramid as a Compact Image Code](http://persci.mit.edu/pub_pdfs/pyramid83.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiscale_detector(fcnn_model, images, image_shapes):\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Next  step\n",
    "\n",
    "Next steps in deep learning detection are R-CNN, Faster R-CNN and SSD architectures.\n",
    "This architecture realization is quite complex.\n",
    "For this reason the task doesn't cover them, but you can find the articles in the internet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
