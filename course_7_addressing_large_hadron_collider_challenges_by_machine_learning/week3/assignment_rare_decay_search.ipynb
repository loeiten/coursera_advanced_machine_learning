{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rare decay search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "from collections import OrderedDict\n",
    "from hep_ml import metrics\n",
    "from utils import check_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hep_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and split into training / test\n",
    "\n",
    "`training.csv` is a mixture of simulated signal, real background.\n",
    "It has the following columns.\n",
    "\n",
    "`test.csv` has the following columns:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ada = pandas.read_csv('reference/training.csv', sep=',')\n",
    "test_ada = pandas.read_csv('reference/test.csv', sep=',', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Training full sample columns:\", \", \".join(train_ada.columns), \"\\nShape:\", train_ada.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Test full sample columns:\", \", \".join(test_ada.columns), \"\\nShape:\", test_ada.shape)\n",
    "test_ada.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train simple model using part of the training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(train_ada, train_size=0.7, test_size=0.3, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's chose features to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = list(set(train_ada.columns) - {'id', 'signal', 'mass', 'production', 'min_ANNmuon'})\n",
    "print (variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.01, random_state=13,\n",
    "                             base_estimator=DecisionTreeClassifier(max_depth=6, \n",
    "                                                                   min_samples_leaf=30,\n",
    "                                                                   max_features=6,\n",
    "                                                                   random_state=13))\n",
    "clf.fit(train[variables], train['signal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check model quality on a half of the training sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Plots the ROC curve\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        The ground-truth\n",
    "    y_pred : array-like\n",
    "        The predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    plt.plot(fpr, tpr, label='ROC AUC=%f' % roc_auc)\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.legend()\n",
    "    plt.title(\"ROC Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict_proba(test[variables])[:, 1]\n",
    "\n",
    "plot_metrics(test['signal'], y_pred)\n",
    "test.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AUC is just a part of the solution, you also have to make sure that\n",
    "\n",
    "- the classifier output is not correlated with the mass\n",
    "- classifier performs similarily on MC and real data of the normalization channel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass correlation check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_check = pandas.read_csv(\"reference/check_correlation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(df_corr_check[variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Never used in original code\n",
    "# NOTE: Uses functions from utils which is not loaded, but declared in the notebook longer down\n",
    "\n",
    "def efficiencies(features, \n",
    "                 thresholds=None, \n",
    "                 mask=None, \n",
    "                 bins=30, \n",
    "                 labels_dict=None,\n",
    "                 ignored_sideband=0.0,\n",
    "                 errors=False,\n",
    "                 grid_columns=2):\n",
    "        \"\"\"\n",
    "        Efficiencies for spectators\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        features : list or None\n",
    "            A list of strings of the features to use\n",
    "            If None, then the classifier's spectators is used\n",
    "        thresholds : list\n",
    "            List of floats of what thresholds to use\n",
    "        mask : None or numbers.Number or array-like or str or function(pandas.DataFrame)\n",
    "            Mask over which data will be used\n",
    "        bins : int or array-like\n",
    "            Bins to use in the histogram\n",
    "        labels_dict : None or OrderedDict\n",
    "            Name for class label\n",
    "            If OrderedDict, the format {int: str} is used\n",
    "            If None then {0: 'bck', '1': 'signal'}\n",
    "        ignored_sideband : float\n",
    "            A float between 0 - 1, where the number indicates the percent of plotting data\n",
    "        errors : bool\n",
    "            If True then use errorbar, else interpolate function\n",
    "        grid_columns : int\n",
    "            Count of columns in the grid\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        plotting.GridPlot\n",
    "        \"\"\"\n",
    "        mask, data, class_labels, weight = self._apply_mask(mask,\n",
    "                                                            self._get_features(features), \n",
    "                                                            self.target, self.weight)\n",
    "        labels_dict = self._check_labels(labels_dict, class_labels)\n",
    " \n",
    "        plots = []\n",
    "        for feature in data.columns:\n",
    "            for name, prediction in self.prediction.items():\n",
    "                prediction = prediction[mask]\n",
    "                eff = OrderedDict()\n",
    "                for label, label_name in labels_dict.items():\n",
    "                    label_mask = class_labels == label\n",
    "                    eff[label_name] = utils.get_efficiencies(prediction[label_mask, label],\n",
    "                                                             data[feature][label_mask].values,\n",
    "                                                             bins_number=bins,\n",
    "                                                             sample_weight=weight[label_mask],\n",
    "                                                             thresholds=thresholds,\n",
    "                                                             errors=errors,\n",
    "                                                             ignored_sideband=ignored_sideband)\n",
    " \n",
    "                for label_name, eff_data in eff.items():\n",
    "                    if errors:\n",
    "                        plot_fig = plotting.ErrorPlot(eff_data)\n",
    "                    else:\n",
    "                        plot_fig = plotting.FunctionsPlot(eff_data)\n",
    "                    plot_fig.xlabel = feature\n",
    "                    plot_fig.ylabel = 'Efficiency for {}'.format(name)\n",
    "                    plot_fig.title = '{} flatness'.format(label_name)\n",
    "                    plot_fig.ylim = (0, 1)\n",
    "                    plots.append(plot_fig)\n",
    " \n",
    "        return plotting.GridPlot(grid_columns, *plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_arrays(*arrays):\n",
    "    \"\"\"\n",
    "    Left for consistency, a version of `sklearn.validation.check_arrays`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arrays : argument-tuple\n",
    "        Input object to check / convert\n",
    "        Arrays with the same length of first dimension\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    checked_arrays : object\n",
    "        The converted and validated array\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(arrays) > 0, 'The number of array must be greater than zero'\n",
    "    checked_arrays = []\n",
    "    shapes = []\n",
    "    for arr in arrays:\n",
    "        if arr is not None:\n",
    "            checked_arrays.append(numpy.array(arr))\n",
    "            shapes.append(checked_arrays[-1].shape[0])\n",
    "        else:\n",
    "            checked_arrays.append(None)\n",
    "    assert numpy.sum(numpy.array(shapes) == shapes[0]) == len(shapes), 'Different shapes of the arrays {}'.format(\n",
    "        shapes)\n",
    "    return checked_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_efficiencies(prediction, \n",
    "                     spectator,\n",
    "                     sample_weight=None,\n",
    "                     bins_number=20,\n",
    "                     thresholds=None,\n",
    "                     errors=False,\n",
    "                     ignored_sideband=0.0):\n",
    "    \"\"\"\n",
    "    Construct efficiency function dependent on spectator for each threshold\n",
    "    \n",
    "    Different score functions available: Efficiency, Precision, Recall, F1Score,\n",
    "    and other things from sklearn.metrics\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction : list\n",
    "        List of probabilities\n",
    "    spectator : list\n",
    "        List of spectator's values\n",
    "    sample_weight : None or array-like\n",
    "        The weight given the of samples\n",
    "    bins_number : int\n",
    "        Count of bins for plot\n",
    "    thresholds : list\n",
    "        List of prediction's threshold\n",
    "        (default=prediction's cuts for which efficiency will be [0.2, 0.4, 0.5, 0.6, 0.8])\n",
    "    errors : bool\n",
    "        Whether or not to include errors\n",
    "    ignored_sidebands : float\n",
    "            A float between 0 - 1, where the number indicates the percent of plotting data \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : OrderedDict\n",
    "        OrderedDict where the keys are the threshold and the values are tuples of arrays of the same length\n",
    "        If errors is False, the values are on the form (x_values, y_values)\n",
    "        If errors is True, the values ar on the form (x_values, y_values, y_err, x_err)\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction, spectator, sample_weight = \\\n",
    "        check_arrays(prediction, spectator, sample_weight)\n",
    "\n",
    "    spectator_min, spectator_max = weighted_quantile(spectator, [ignored_sideband, (1. - ignored_sideband)])\n",
    "    mask = (spectator >= spectator_min) & (spectator <= spectator_max)\n",
    "    spectator = spectator[mask]\n",
    "    prediction = prediction[mask]\n",
    "    bins_number = min(bins_number, len(prediction))\n",
    "    sample_weight = sample_weight if sample_weight is None else numpy.array(sample_weight)[mask]\n",
    "\n",
    "    if thresholds is None:\n",
    "        thresholds = [weighted_quantile(prediction, quantiles=1 - eff, sample_weight=sample_weight)\n",
    "                      for eff in [0.2, 0.4, 0.5, 0.6, 0.8]]\n",
    "\n",
    "    binner = Binner(spectator, bins_number=bins_number)\n",
    "    if sample_weight is None:\n",
    "        sample_weight = numpy.ones(len(prediction))\n",
    "    bins_data = binner.split_into_bins(spectator, prediction, sample_weight)\n",
    "\n",
    "    bin_edges = numpy.array([spectator_min] + list(binner.limits) + [spectator_max])\n",
    "    xerr = numpy.diff(bin_edges) / 2.\n",
    "    result = OrderedDict()\n",
    "    for threshold in thresholds:\n",
    "        x_values = []\n",
    "        y_values = []\n",
    "        N_in_bin = []\n",
    "        for num, (masses, probabilities, weights) in enumerate(bins_data):\n",
    "            y_values.append(numpy.average(probabilities > threshold, weights=weights))\n",
    "            N_in_bin.append(numpy.sum(weights))\n",
    "            if errors:\n",
    "                x_values.append((bin_edges[num + 1] + bin_edges[num]) / 2.)\n",
    "            else:\n",
    "                x_values.append(numpy.mean(masses))\n",
    "\n",
    "        x_values, y_values, N_in_bin = check_arrays(x_values, y_values, N_in_bin)\n",
    "        if errors:\n",
    "            result[threshold] = (x_values, y_values, numpy.sqrt(y_values * (1 - y_values) / N_in_bin), xerr)\n",
    "        else:\n",
    "            result[threshold] = (x_values, y_values)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_quantile(array, \n",
    "                      quantiles,\n",
    "                      sample_weight=None,\n",
    "                      array_sorted=False,\n",
    "                      old_style=False):\n",
    "    \"\"\"\n",
    "    Computing quantiles of an array. \n",
    "    \n",
    "    Unlike the numpy.percentile, this function supports weights,\n",
    "    but it is inefficient and performs complete sorting.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array : array, shape (n_samples,)\n",
    "        The input distribution\n",
    "    quantiles : array-like, shape (n_quantiles,)\n",
    "        Array of floats from range [0, 1] with quantiles of shape \n",
    "    sample_weight : None or array-like, shape (n_samples,)\n",
    "        Optional weights of the samples\n",
    "    array_sorted : bool\n",
    "        If True, the sorting step will be skipped\n",
    "    old_style : bool\n",
    "        If True, will correct output to be consistent with numpy.percentile.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array, shape (n_quantiles,)\n",
    "        The values of the percentiles\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> weighted_quantile([1, 2, 3, 4, 5], [0.5])\n",
    "    array([ 3.])\n",
    "    >>> weighted_quantile([1, 2, 3, 4, 5], [0.5], sample_weight=[3, 1, 1, 1, 1])\n",
    "    array([ 2.])\n",
    "    \"\"\"\n",
    "    \n",
    "    array = numpy.array(array)\n",
    "    quantiles = numpy.array(quantiles)\n",
    "    sample_weight = check_sample_weight(array, sample_weight)\n",
    "    assert numpy.all(quantiles >= 0) and numpy.all(quantiles <= 1), 'Percentiles should be in [0, 1]'\n",
    "\n",
    "    if not array_sorted:\n",
    "        array, sample_weight = reorder_by_first(array, sample_weight)\n",
    "\n",
    "    weighted_quantiles = numpy.cumsum(sample_weight) - 0.5 * sample_weight\n",
    "    if old_style:\n",
    "        # To be convenient with numpy.percentile\n",
    "        weighted_quantiles -= weighted_quantiles[0]\n",
    "        weighted_quantiles /= weighted_quantiles[-1]\n",
    "    else:\n",
    "        weighted_quantiles /= numpy.sum(sample_weight)\n",
    "    return numpy.interp(quantiles, weighted_quantiles, array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sample_weight(y_true, sample_weight):\n",
    "    \"\"\"\n",
    "    Asserts that the weights and predictions have the same length\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like, shape (n_samples,)\n",
    "        The ground-truth\n",
    "    sample_weight : None or array-like, shape (n_samples,)\n",
    "        The assigned weights\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array-like, shape (n_samples,)\n",
    "        The input sample_weight if input sample_weight is not None\n",
    "        An array of ones else\n",
    "    \"\"\"\n",
    "    \n",
    "    if sample_weight is None:\n",
    "        return numpy.ones(len(y_true), dtype=numpy.float)\n",
    "    else:\n",
    "        sample_weight = numpy.array(sample_weight, dtype=numpy.float)\n",
    "        assert len(y_true) == len(sample_weight), \\\n",
    "            \"The length of weights is different: not {0}, but {1}\".format(len(y_true), len(sample_weight))\n",
    "        return sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_by_first(*arrays):\n",
    "    \"\"\"\n",
    "    Applies the same permutation to all passed arrays.\n",
    "    \n",
    "    The order of the permutation is passed as the first array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arrays : argument-tuple\n",
    "        The first element in arrays must be the order\n",
    "        The arrays must have the same length of first dimension\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of the input arrays, ordered by the first input array\n",
    "    \"\"\"\n",
    "    \n",
    "    arrays = check_arrays(*arrays)\n",
    "    order = numpy.argsort(arrays[0])\n",
    "    return [arr[order] for arr in arrays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binner(object):\n",
    "    \"\"\"\n",
    "    Class that helps to split the values into several bins.\n",
    "    \n",
    "    Initially an array of values is given, which is then splitted into 'bins_number' equal parts,\n",
    "    and thus we are computing limits (boundaries of bins).\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, values, bins_number):\n",
    "        \"\"\"\n",
    "        Class constructor\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        values : array-like\n",
    "            The input distribution\n",
    "        bins_number : int\n",
    "            Count of bins for plot\n",
    "        \"\"\"\n",
    "        \n",
    "        percentiles = [i * 100.0 / bins_number for i in range(1, bins_number)]\n",
    "        self.limits = numpy.percentile(values, percentiles)\n",
    "\n",
    "    def get_bins(self, values):\n",
    "        \"\"\"\n",
    "        Given the values of feature, compute the index of bin\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        values : array-like, shape (n_samples,)\n",
    "            The values to get the bin number from\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            The bin numbers\n",
    "        \"\"\"\n",
    "        \n",
    "        return numpy.searchsorted(self.limits, values)\n",
    "\n",
    "    def set_limits(self, limits):\n",
    "        \"\"\"Change the thresholds inside bins\"\"\"\n",
    "        self.limits = limits\n",
    "\n",
    "    # NOTE: Explaination of property decorator\n",
    "    #       https://stackoverflow.com/questions/17330160/how-does-the-property-decorator-work\n",
    "    @property\n",
    "    def bins_number(self):\n",
    "        \"\"\"\n",
    "        Returns the number of bins\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The total number of bins\n",
    "        \"\"\"\n",
    "        return len(self.limits) + 1\n",
    "\n",
    "    def split_into_bins(self, *arrays):\n",
    "        \"\"\"\n",
    "        Split data into bins\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        array : argument-tuple\n",
    "            Data to be splitted\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        results : list, shape (n_bins,)\n",
    "            Values corresponding to each bin.\n",
    "        \"\"\"\n",
    "        \n",
    "        values = arrays[0]\n",
    "        for array in arrays:\n",
    "            assert len(array) == len(values), \"passed arrays have different length\"\n",
    "        bins = self.get_bins(values)\n",
    "        result = []\n",
    "        for b in range(len(self.limits) + 1):\n",
    "            indices = bins == b\n",
    "            result.append([numpy.array(array)[indices] for array in arrays])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff = get_efficiencies(y_pred, df_corr_check.mass, thresholds=[0.5]) #, thresholds=[0.2, 0.4, 0.5, 0.6, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_name, eff_data in eff.items():\n",
    "    pyplot.plot(eff_data[0], eff_data[1], label=\"global eff  %.1f\" % label_name)\n",
    "    \n",
    "pyplot.xlabel('mass')\n",
    "pyplot.ylabel('Efficiency')\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_metric = check_correlation(y_pred, df_corr_check['mass'])\n",
    "print (corr_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC vs Real difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agreement = pandas.read_csv('reference/check_agreement.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Never used in original code\n",
    "\n",
    "def get_ks_metric(df_agree, df_test):\n",
    "    \"\"\"\n",
    "    Returns the Kolmogorov-Smirnov (ks) metric\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    df_agree : DataFrame\n",
    "        A dataframe containing the agreement data\n",
    "    df_test : DataFrame\n",
    "        A dataframe containing the test data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Series\n",
    "        The series containing the ks distance\n",
    "    \"\"\"\n",
    "    \n",
    "    sig_ind = df_agree[df_agree['signal'] == 1].index\n",
    "    bck_ind = df_agree[df_agree['signal'] == 0].index\n",
    "\n",
    "    mc_prob = numpy.array(df_test.loc[sig_ind]['prediction'])\n",
    "    mc_weight = numpy.array(df_agree.loc[sig_ind]['weight'])\n",
    "    data_prob = numpy.array(df_test.loc[bck_ind]['prediction'])\n",
    "    data_weight = numpy.array(df_agree.loc[bck_ind]['weight'])\n",
    "    val, agreement_metric = check_agreement_ks_sample_weighted(data_prob, mc_prob, data_weight, mc_weight)\n",
    "    \n",
    "    return agreement_metric['ks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Never used in original code\n",
    "\n",
    "def check_agreement_ks_sample_weighted(data_prediction,\n",
    "                                       mc_prediction,\n",
    "                                       weights_data,\n",
    "                                       weights_mc):\n",
    "    \"\"\"\n",
    "    Checks the agreement between the data prediction and monte carlo prediction\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_prediction : array-like\n",
    "        Predictions from the data\n",
    "    mc_prediction : array-like\n",
    "        Predictions from the Monte Carlo simulations\n",
    "    weights_data : array-like\n",
    "        Weights for the real data\n",
    "    weights_mc : array-like\n",
    "        Wight for the Monte Carlo simulation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        Whether or not the ks distance part is less than 0.03\n",
    "    result : Dict\n",
    "        Dictionary on the form {'ks': ks_distance, 'ks_part': ks_distance_part}\n",
    "    \"\"\"\n",
    "    \n",
    "    data_prediction, weights_data = map(column_or_1d, [data_prediction, weights_data])\n",
    "    mc_prediction, weights_mc = map(column_or_1d, [mc_prediction, weights_mc])\n",
    "\n",
    "    assert numpy.all(data_prediction >= 0.) and numpy.all(data_prediction <= 1.), 'error in prediction'\n",
    "    assert numpy.all(mc_prediction >= 0.) and numpy.all(mc_prediction <= 1.), 'error in prediction'\n",
    "\n",
    "    weights_data = weights_data / numpy.sum(weights_data)\n",
    "    weights_mc = weights_mc / numpy.sum(weights_mc)\n",
    "\n",
    "    data_neg = data_prediction[weights_data < 0]\n",
    "    weights_neg = -weights_data[weights_data < 0]\n",
    "    mc_prediction = numpy.concatenate((mc_prediction, data_neg))\n",
    "    weights_mc = numpy.concatenate((weights_mc, weights_neg))\n",
    "    data_prediction = data_prediction[weights_data >= 0]\n",
    "    weights_data = weights_data[weights_data >= 0]\n",
    "\n",
    "    assert numpy.all(weights_data >= 0) and numpy.all(weights_mc >= 0)\n",
    "    assert numpy.allclose(weights_data.sum(), weights_mc.sum())\n",
    "\n",
    "    weights_data /= numpy.sum(weights_data)\n",
    "    weights_mc /= numpy.sum(weights_mc)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve_splitted(data_prediction, mc_prediction, weights_data, weights_mc)\n",
    "\n",
    "    Dnm = numpy.max(numpy.abs(fpr - tpr))\n",
    "    Dnm_part = numpy.max(numpy.abs(fpr - tpr)[fpr + tpr < 1])\n",
    "\n",
    "    result = {'ks': Dnm, 'ks_part': Dnm_part}\n",
    "    return Dnm_part < 0.03, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agreement.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agreement[variables].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ks(data_prediction, mc_prediction, weights_data, weights_mc):\n",
    "    \"\"\"\n",
    "    Compute Kolmogorov-Smirnov (ks) distance between real data predictions cdf and Monte Carlo one.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_prediction : array-like\n",
    "        The real data predictions\n",
    "    mc_prediction : array-like\n",
    "        The Monte Carlo data predictions\n",
    "    weights_data : array-like\n",
    "        The real data weights\n",
    "    weights_mc : array-like\n",
    "        The Monte Carlo weights\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dnm : float\n",
    "        The ks distance\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(data_prediction) == len(weights_data), 'Data length and weight one must be the same'\n",
    "    assert len(mc_prediction) == len(weights_mc), 'Data length and weight one must be the same'\n",
    "\n",
    "    data_prediction, mc_prediction = numpy.array(data_prediction), numpy.array(mc_prediction)\n",
    "    weights_data, weights_mc = numpy.array(weights_data), numpy.array(weights_mc)\n",
    "\n",
    "    assert numpy.all(data_prediction >= 0.) and numpy.all(data_prediction <= 1.), 'Data predictions are out of range [0, 1]'\n",
    "    assert numpy.all(mc_prediction >= 0.) and numpy.all(mc_prediction <= 1.), 'MC predictions are out of range [0, 1]'\n",
    "\n",
    "    weights_data /= numpy.sum(weights_data)\n",
    "    weights_mc /= numpy.sum(weights_mc)\n",
    "\n",
    "    fpr, tpr = __roc_curve_splitted(data_prediction, mc_prediction, weights_data, weights_mc)\n",
    "\n",
    "    Dnm = numpy.max(numpy.abs(fpr - tpr))\n",
    "    return Dnm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __roc_curve_splitted(data_zero, data_one, sample_weights_zero, sample_weights_one):\n",
    "    \"\"\"\n",
    "    Compute the roc curve with sample weights\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_zero : array-like\n",
    "        Data labeled with 0\n",
    "    data_one : array-like\n",
    "        Data labeled with 1\n",
    "    sample_weights_zero : array-like \n",
    "        Weights for 0-labeled data\n",
    "    sample_weights_one : array-like\n",
    "        Weights for 1-labeled data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fpr : np.array\n",
    "        The false positive rate\n",
    "    tpr : np.array\n",
    "        The true positive rate\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = [0] * len(data_zero) + [1] * len(data_one)\n",
    "    weights = numpy.concatenate([sample_weights_zero, sample_weights_one])\n",
    "    data_all = numpy.concatenate([data_zero, data_one])\n",
    "    fpr, tpr, _ = roc_curve(labels, data_all, sample_weight=weights)\n",
    "    \n",
    "    return fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_probs = clf.predict_proba(df_agreement[variables])[:, 1]\n",
    "\n",
    "ks = compute_ks(agreement_probs[df_agreement['signal'].values == 0],\n",
    "                agreement_probs[df_agreement['signal'].values == 1],\n",
    "                df_agreement[df_agreement['signal'] == 0]['weight'].values,\n",
    "                df_agreement[df_agreement['signal'] == 1]['weight'].values)\n",
    "\n",
    "print ('KS metric:', ks, \"is OK:\", ks < 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ks(X_agreement, y_pred):\n",
    "    \"\"\"\n",
    "    Plot the prediction distribution\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_agreement : DataFrame\n",
    "        DataFrame with the agreement data\n",
    "        Must include the column \"signal\"\n",
    "    y_pred : array-like\n",
    "        The prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    sig_ind = X_agreement[X_agreement['signal'] == 1].index\n",
    "    bck_ind = X_agreement[X_agreement['signal'] == 0].index\n",
    "\n",
    "    mc_prob = y_pred[sig_ind]\n",
    "    mc_weight = numpy.array(X_agreement.loc[sig_ind]['weight'])\n",
    "    \n",
    "    data_prob = y_pred[bck_ind]\n",
    "    data_weight = numpy.array(X_agreement.loc[bck_ind]['weight'])\n",
    "    \n",
    "    inds = data_weight < 0\n",
    "    \n",
    "    mc_weight = numpy.array(list(mc_weight) + list(-data_weight[inds]))\n",
    "    mc_prob = numpy.array(list(mc_prob) + list(data_prob[inds]))\n",
    "    \n",
    "    data_prob = data_prob[data_weight >= 0]\n",
    "    data_weight = data_weight[data_weight >= 0]\n",
    "    \n",
    "    hist(data_prob, weights=data_weight, color='r', histtype='step', density=True, bins=60, label='data')\n",
    "    hist(mc_prob, weights=mc_weight, color='b', histtype='step', density=True, bins=60, label='mc')\n",
    "    \n",
    "    xlabel(\"prediction\")\n",
    "    legend(loc=2)\n",
    "    \n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ks(df_agreement, agreement_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see if adding some noise can improve the agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(array, level=0.40, random_seed=34):\n",
    "    \"\"\"\n",
    "    Adds ramdom noise to the input array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array : array-like\n",
    "        The array to add noise to\n",
    "    level : float\n",
    "        The signal portion of the signal/noise ratio\n",
    "    random_seed : int\n",
    "        The random seed to use\n",
    "    \"\"\"\n",
    "    \n",
    "    numpy.random.seed(random_seed)\n",
    "    \n",
    "    return level * numpy.random.random(size=array.size) + (1 - level) * array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_probs_noise = add_noise(clf.predict_proba(df_agreement[variables])[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_noise = compute_ks(agreement_probs_noise[df_agreement['signal'].values == 0],\n",
    "                      agreement_probs_noise[df_agreement['signal'].values == 1],\n",
    "                      df_agreement[df_agreement['signal'] == 0]['weight'].values,\n",
    "                      df_agreement[df_agreement['signal'] == 1]['weight'].values)\n",
    "\n",
    "print ('KS metric:', ks_noise, \"is OK:\", ks_noise < 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ks(df_agreement, agreement_probs_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check ROC with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = add_noise(clf.predict_proba(test[variables])[:, 1])\n",
    "\n",
    "plot_metrics(test['signal'], y_pred)\n",
    "test.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model using the whole training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(train_ada[variables], train_ada['signal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute prediction and add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = add_noise(clf.predict_proba(test_ada[variables])[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_submission(y_pred, index, filename='result'):\n",
    "    \"\"\"\n",
    "    Saves the submission to a csv.gz file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : array-like\n",
    "        The prediction\n",
    "    index : array-like\n",
    "        The id-index corresponding to the prediction\n",
    "    filename : str\n",
    "        The base name of the submission file (i.e. excluding the extension)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filename : str\n",
    "        The file name of the submission file\n",
    "    \"\"\"\n",
    "    \n",
    "    sep = ','\n",
    "    filename = '{}.csv.gz'.format(filename)\n",
    "    pandas.DataFrame({'id': index, \n",
    "                      'prediction': y_pred}).to_csv(filename, \n",
    "                                                    sep=sep, \n",
    "                                                    index=False,\n",
    "                                                    compression='gzip')\n",
    "    print (\"Saved file: \", filename, \"\\nShape:\", (y_pred.shape[0], 2))\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_submission(y_pred, test_ada.index, \"sample_submission\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
