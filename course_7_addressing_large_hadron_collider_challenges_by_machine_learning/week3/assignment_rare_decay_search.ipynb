{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Rare decay search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "!pip install hep_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Load dataset and split into training / test\n",
    "\n",
    "`training.csv` is a mixture of simulated signal, real background.\n",
    "It has the following columns.\n",
    "\n",
    "`test.csv` has the following columns:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "train_ada = pandas.read_csv('reference/training.csv', sep=',')\n",
    "test_ada = pandas.read_csv('reference/test.csv', sep=',', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print (\"Training full sample columns:\", \", \".join(train_ada.columns), \"\\nShape:\", train_ada.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print (\"Test full sample columns:\", \", \".join(test_ada.columns), \"\\nShape:\", test_ada.shape)\n",
    "test_ada.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Train simple model using part of the training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(train_ada, train_size=0.7, test_size=0.3, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Let's chose features to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "variables = list(set(train_ada.columns) - {'id', 'signal', 'mass', 'production', 'min_ANNmuon'})\n",
    "print (variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = AdaBoostClassifier(n_estimators=150, \n",
    "                         learning_rate=0.009, \n",
    "                         random_state=13,\n",
    "                         base_estimator=DecisionTreeClassifier(max_depth=20, \n",
    "                                                               min_samples_leaf=40,\n",
    "                                                               max_features=10,\n",
    "                                                               random_state=13))\n",
    "clf.fit(train[variables], train['signal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Check model quality on a half of the training sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def plot_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Plots the ROC curve\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        The ground-truth\n",
    "    y_pred : array-like\n",
    "        The predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    plt.plot(fpr, tpr, label='ROC AUC=%f' % roc_auc)\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.legend()\n",
    "    plt.title(\"ROC Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict_proba(test[variables])[:, 1]\n",
    "\n",
    "plot_metrics(test['signal'], y_pred)\n",
    "test.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "ROC AUC is just a part of the solution, you also have to make sure that\n",
    "\n",
    "- the classifier output is not correlated with the mass\n",
    "- classifier performs similarily on MC and real data of the normalization channel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Mass correlation check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "df_corr_check = pandas.read_csv(\"reference/check_correlation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "df_corr_check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(df_corr_check[variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# NOTE: In the original file, a routine called `efficiencies` was defined here\n",
    "#       This was however never used, and were refering to unreferenced variables, including self.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "eff = utils.get_efficiencies(y_pred, df_corr_check.mass, thresholds=[0.5]) #, thresholds=[0.2, 0.4, 0.5, 0.6, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "eff.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "for label_name, eff_data in eff.items():\n",
    "    pyplot.plot(eff_data[0], eff_data[1], label=\"global eff  %.1f\" % label_name)\n",
    "    \n",
    "pyplot.xlabel('mass')\n",
    "pyplot.ylabel('Efficiency')\n",
    "pyplot.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "corr_metric = utils.check_correlation(y_pred, df_corr_check['mass'])\n",
    "print (corr_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## MC vs Real difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "df_agreement = pandas.read_csv('reference/check_agreement.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "df_agreement.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "df_agreement[variables].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "agreement_probs = clf.predict_proba(df_agreement[variables])[:, 1]\n",
    "\n",
    "ks = utils.compute_ks(agreement_probs[df_agreement['signal'].values == 0],\n",
    "                      agreement_probs[df_agreement['signal'].values == 1],\n",
    "                      df_agreement[df_agreement['signal'] == 0]['weight'].values,\n",
    "                      df_agreement[df_agreement['signal'] == 1]['weight'].values)\n",
    "\n",
    "print ('KS metric:', ks, \"is OK:\", ks < 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def plot_ks(X_agreement, y_pred):\n",
    "    \"\"\"\n",
    "    Plot the prediction distribution\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_agreement : DataFrame\n",
    "        DataFrame with the agreement data\n",
    "        Must include the column \"signal\"\n",
    "    y_pred : array-like\n",
    "        The prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    sig_ind = X_agreement[X_agreement['signal'] == 1].index\n",
    "    bck_ind = X_agreement[X_agreement['signal'] == 0].index\n",
    "\n",
    "    mc_prob = y_pred[sig_ind]\n",
    "    mc_weight = numpy.array(X_agreement.loc[sig_ind]['weight'])\n",
    "    \n",
    "    data_prob = y_pred[bck_ind]\n",
    "    data_weight = numpy.array(X_agreement.loc[bck_ind]['weight'])\n",
    "    \n",
    "    inds = data_weight < 0\n",
    "    \n",
    "    mc_weight = numpy.array(list(mc_weight) + list(-data_weight[inds]))\n",
    "    mc_prob = numpy.array(list(mc_prob) + list(data_prob[inds]))\n",
    "    \n",
    "    data_prob = data_prob[data_weight >= 0]\n",
    "    data_weight = data_weight[data_weight >= 0]\n",
    "    \n",
    "    hist(data_prob, weights=data_weight, color='r', histtype='step', density=True, bins=60, label='data')\n",
    "    hist(mc_prob, weights=mc_weight, color='b', histtype='step', density=True, bins=60, label='mc')\n",
    "    \n",
    "    xlabel(\"prediction\")\n",
    "    legend(loc=2)\n",
    "    \n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "plot_ks(df_agreement, agreement_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Let's see if adding some noise can improve the agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "agreement_probs_noise = utils.add_noise(clf.predict_proba(df_agreement[variables])[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "ks_noise = utils.compute_ks(agreement_probs_noise[df_agreement['signal'].values == 0],\n",
    "                            agreement_probs_noise[df_agreement['signal'].values == 1],\n",
    "                            df_agreement[df_agreement['signal'] == 0]['weight'].values,\n",
    "                            df_agreement[df_agreement['signal'] == 1]['weight'].values)\n",
    "\n",
    "print ('KS metric:', ks_noise, \"is OK:\", ks_noise < 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "plot_ks(df_agreement, agreement_probs_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Check ROC with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "y_pred = utils.add_noise(clf.predict_proba(test[variables])[:, 1])\n",
    "\n",
    "plot_metrics(test['signal'], y_pred)\n",
    "test.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Train the model using the whole training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clf.fit(train_ada[variables], train_ada['signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ada_probs = clf.predict_proba(train_ada[variables])[:, 1]\n",
    "plot_metrics(train_ada['signal'], train_ada_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Compute prediction and add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "y_ada = clf.predict_proba(test_ada[variables])[:, 1]\n",
    "y_pred = utils.add_noise(y_ada, level=0.17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Prepare submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def save_submission(y_pred, index, filename='result'):\n",
    "    \"\"\"\n",
    "    Saves the submission to a csv.gz file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : array-like\n",
    "        The prediction\n",
    "    index : array-like\n",
    "        The id-index corresponding to the prediction\n",
    "    filename : str\n",
    "        The base name of the submission file (i.e. excluding the extension)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filename : str\n",
    "        The file name of the submission file\n",
    "    \"\"\"\n",
    "    \n",
    "    sep = ','\n",
    "    filename = '{}.csv.gz'.format(filename)\n",
    "    pandas.DataFrame({'id': index, \n",
    "                      'prediction': y_pred}).to_csv(filename, \n",
    "                                                    sep=sep, \n",
    "                                                    index=False,\n",
    "                                                    compression='gzip')\n",
    "    print (\"Saved file: \", filename, \"\\nShape:\", (y_pred.shape[0], 2))\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "save_submission(y_pred, test_ada.index, \"sample_submission\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
